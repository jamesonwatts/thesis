\chapter{Legitimacy and Language\label{lang}}

\begin{small}
\begin{quote}
``Grammars in both cooking and engineering exist not just as rules but as a set of unspoken practices taken for granted" (Arthur 2009 p. 77)
\end{quote}
\end{small}
Chapter \ref{law} introduced a concept called `market entropy' as a way to describe the degree to which actors are similar or different in their representations of the market. This is a broad concept which incorporates insights from several established literatures published under the neo-institutionalist moniker (e.g. Denzau and North 1994; DiMaggio and Powell 1983; Loasby 1999). In this chapter, I narrow the scope of investigation slightly to focus on one of these literatures; namely, research on legitimacy and the legitimation process. 

Legitimacy can be defined as ``consensus among agents (audiences) that the features and activities of [market actors] are appropriate and desirable within a widespread, taken-for-granted system of norms or social codes" (Cattani et al. 2008, p. 147). Legitimacy is distinct from the more general `market entropy' construct in that it imposes a moral dimension on shared understanding--that is, behavior is not just `taken-for-granted' but also `appropriate.' However, research on the legitimation \emph{process} can help us understand how shared understandings actually come about in practice (Colyvas and Powell 2008).

Typically, legitimation occurs over a long period of time and involves many actors (Rosa et al. 1999). The process eventually leads to the establishment of field-wide knowledge and shared understanding of the boundaries and definition of relevant technological dimensions (Bijker, Hughes, and Pinch, 1987; Santos and Eisenhardt, 2009), the meaning of new product categories (Kennedy, Lo, and Lounsbury, 2010; Rosa et al., 1999), and the validity of new practices (Lounsbury and Crumley, 2007; Tripsas, 2009). However, as the legitimation process unfolds, there are often periods of high disagreement (e.g. Colyvas and Powell 2008) during which actor's representations of the market are either in conflict or not sufficiently diffuse.

This formulation of shared understanding--derived from consideration of the legitimation process--comports nicely with my focus on field-level transitions in technology, taste and attention. However, a brief review of the empirical literature on legitimacy raises an additional concern: the behavior deemed legitimate is typically defined ex post--that is, operationalizations are based on the present incarnation of a category and then traced to some earlier period. For example, Petkova et al., (2014) examine the legitimacy of investing in the clean energy sector by counting historical references to the terms ``clean energy," ``green energy" and ``alternative energy" in media articles. In practice, categories emerge organically (Goldberg 2013) and may cycle through several incarnations of appropriate behavior before they settle on the perspicuously demarcated boundaries that appear to the present observer. More importantly, this formulation excludes the struggle between competing paradigms that characterizes the typical legitimation process (Colyvas and Powell 2008).

How then, do we attempt to understand transitions in technology, taste and attention in an unbiased manner? Despite it's shortcomings, the literature on legitimacy provides several useful insights. Foremost, the frequency of  behavior is a meaningful yardstick of general acceptance (Hannan and Carroll, 1992). Second, the frequencies of language used in the media are representative of an unfolding legitimation process (Petkova et al., 2014; Rosa et al., 1999). Using these insights, I propose a general notion of shared understanding based on changes in the frequency of language use. This can be accomplished without imposing ex post classifications by tracking language frequencies as they naturally occur--that is, the frequency distribution of descriptive words at various points in time. Changes in a frequency distribution from one period to the next is thus assumed to represent shifts in the importance of behavior described by the language. In contrast, similar distributions from one period to the next may represent an emerging consensus in the field. 

\section{Language Models}

A variety of recent studies have started to look at the generic characteristics of written language as a way to understand the structure of markets and the evolution of human behavior (Klingenstein, Hitchcock, and DeDeo 2014; Goldberg 2013; Tirunillai and Tellis 2014). The viability of this type of research is driven by parallel developments in the availability of digital archives and methods appropriate for the analysis of ``big data." While the standards and practices used for large-scale textual analysis are still very much under development, I have largely followed the work of prior scholars in preparing my data and constructing my measures. The following paragraphs describe this process in detail.

Textual data is sourced from articles written for the Bioworld trade journal between January 1991 and December 2004. During this period, Bioworld constituted a primary source for industry-wide dissemination of information about the activities of firms and other stakeholders in the biotechnology industry. They published daily articles (excluding weekends) on topics as wide-ranging as personnel changes, fundraising activity, inter-firm contracting, the FDA approval process and the latest scientific trends. Below are two different example articles--the first from 1994 and the second from 2004--that illustrate the level of detail typically present in the text.

\begin{singlespace}
\begin{small}
\begin{quotation}
\noindent \textbf{Teaching Cells To Make Their Own Anti-Aids Interferon} \\

It's one thing to administer recombinant interferon as antiviral therapy. Educating cells to make their own do-it-yourself interferon is something else. A team of French scientists reports doing just that to hamstring the AIDS virus. Their paper in the current Proceedings of the National Academy of Sciences (PNAS) describes: ``Blocking of retroviral infection at a step prior to reverse transcription in cells transformed to constitutively express interferon b." The group's leader, Edward DeMaeyer of France's National Scientific Research Institute (INRA) stated, ``We are developing methods for somatic-cell gene therapy directed against infection with human immunodeficiency virus ...through the constitutive production of autocrine interferon (IFN)." His strategy aims to block HIV at its earliest stages of cell entry and replication, rather than the later stages at which interferon is commonly thought to act.

For starters, the team transfected various mouse and human cell lines with IFN genes. These transformed cells synthesized interferon at a constant low rate; their replication and survival were the same as in control cells. When challenged with various retroviruses, they markedly reduced the number of virally infected cells. Then the team assailed CEM cells with HIV viral particles. (CEM is a model human T-cell line derived from patients with leukemia.) ``Two days later," as DeMaeyer reported, ``the presence of HIV-1 proviral DNA was significantly lower in the human IFN-b transformed cells than in the control cells, indicating an early block of the infectious cycle."

Six hours after the onset of infection, 40 to 80 percent of the viral particles could still be found in the transformed cells' culture medium, whereas 90 percent of the control cells' virions had moved on to infect other cells. But the transformed CEM cells died within a week or so. ``These findings," DeMaeyer concluded, are encouraging for the use of the ...IFN-b vector to explore the possibility of developing an anti-HIV-b-directed somatic cell gene therapy." Richard Mulligan at MIT's Whitehead Institute of Biomedical Research supplied the vector plasmids to the French investigators.

SInce submitting his findings to PNAS last September, DeMaeyer has determined that virus-resistant, inferferon-synthesizing CEM cells ``survive for as long as 80 days. Maybe they lived even longer," he told BioWorld in a telephone interview, ``but that's when we stopped the experiment." Moreover, moving from off-the-shelf CEM cell lines, he and his group have shown ``that the same effect is obtained in fresh peripheral blood lymphocytes taken from people. Of course," he added, "this is more
significant in terms of somatic gene therapy. That's the cell that one wants eventually to protect in the body."

The next step, he said, is to proceed from in vitro to in vivo testing. ``We want to collaborate with a group that has the rhesus monkey model going, infectable with the simian immune deficiency virus, SIV." He is already in discussion with one such research center, but is still open to other offers.

DeMaeyer cited three goals of such testing: First, to show that it works in vivo; second, to determine that the transformed cell's
immunological function is not impaired; third, to solve the problem of ``getting enough of those interferon-transformed, virus-resistant cells into the body of a seropositive person infected with AIDS."\\

\noindent \textbf{Threshold Pharmaceuticals Inc. began patient enrollment in a Phase III trial of glufosfamide for pancreatic cancer.} \\

The pivotal study is designed to test the company's lead product in patients with metastatic pancreatic cancer refractory to first-line treatment. Its primary endpoint will compare the median survival of patients treated with glufosfamide and best supportive care to those solely receiving best supportive care - all medical or surgical interventions that a pancreatic cancer patient should receive to palliate the cancer but excluding treatment with systemic therapies intended to kill the cancer cells.

At present, there are no approved therapies for those patients, who generally have an expected survival of about three months. ``In fact, one would really say there are no proven options for systemic treatment of their cancer," Threshold President George Tidmarsh told \emph{BioWorld Today}. Of the 31,860 patients expected to be diagnosed with pancreatic cancer in the U.S. this year, about 31,270 will die from the disease, according to American Cancer Society statistics.

The South San Francisco-based company agreed with the FDA on a special protocol assessment, which indicates that if the trial successfully meets its primary endpoint, the data will provide support for an efficacy claim in an eventual marketing application. Tidmarsh said the FDA requested only a few minor wording changes to Threshold's study design, case-reporting system and statistical analyses plans as both parties came to an agreement in the process.

Enrollment is under way in the U.S., while more sites will be added in Eastern Europe and South America. Eventually the trial will involve about 306 patients; recruitment is expected to last between 12 months and 15 months. Eligible patients will be randomized to receive glufosfamide every three weeks in addition to best supportive care, or best supportive care alone.

``There is a planned analysis on an interim data set that we project to have in about 15 months," Tidmarsh said, noting that the timeline would depend on enrollment. ``Our anticipation is that the complete data, with all finalized study reports, would be available in approximately 18 to 24 months."

Secondary endpoints in the trial include disease-free survival and time to disease progression, as well as response and safety evaluations. A next-generation chemotherapeutic agent from the alkylator class, it is designed to enter cells through up-regulated glucose transport proteins.

``The toxin is attached to the glucose molecule, so we believe that it allows the drug to target specifically to the cancer through a mechanism we call metabolic targeting," Tidmarsh said. ``And metabolic targeting is based on the dramatic increased consumption of glucose that tumor cells have relative to normal cells."

He added that glufosfamide produces fewer side effects than prior-generation alkylators, which generated toxins that led to hemorrhagic cystitis. Prior studies have provided a glimpse at the drug's potential efficacy in fighting pancreatic cancer.

Data from a Phase II trial, reported at the America Association for Cancer Research meeting in 2002 and published in the November 2003 issue of the \emph{European Journal of Cancer}, revealed objective tumor shrinkage. An updated analysis of the survival shows an estimated 9 percent two-year survival, which the company said compares to 1 percent or less in historical studies with other first-line therapies. The fully enrolled study included 35 chemotherapyna ve patients with locally advanced or metastatic pancreatic cancer.  

Findings from an initial Phase I trial, published in the October 2000 edition of the \emph{Journal of Clinical Oncology}, showed that its only pancreatic cancer participant achieved a complete remission of disease and remained in remission more than five years later after receiving glufosfamide treatment alone.

Threshold owns all rights to the compound, and Tidmarsh said the company would continue to steer development on its own. It also has demonstrated glufosfamide's activity in Phase II studies in breast and colon cancers, findings which were reported at this year's American Society of Clinical Oncology meeting, and plans to explore its use in lymphoma.

Outside of glufosfamide's development, Threshold is developing two other clinical-stage products, both of which also are based on the company's metabolic targeting approach. A compound labeled TH-070 is in Phase II for benign prostatic hyperplasia, while its 2-deoxyglucose product is in Phase I for solid tumors.

Earlier this year, Threshold filed to go public, though its initial public offering has yet to price. (See \emph{BioWorld Today}, April 15, 2004.) 
\end{quotation}
\end{small}
\end{singlespace}

With even a quick glance at the prior two articles, one will notice several prominent features of the text. First, there are a large number of industry-specific words dealing with the science of biotechnology--words like `glufosfamide' and `virion.' These words are very specific and exist at a very low frequency in the text. Because of this, they provide little insight into the broad subject matter of the article. In contrast, words like `the,' `to,' `is,' `of,' and `was' show up everywhere. However, these words constitute the ``plumbing" of the english language and are also uninformative. Between these two extremes, are words like `HIV,' `cancer' and `cell.' These words are descriptive, but not overly specific. Moreover, they appear with sufficient periodicity to suggest they are an integral part of subject matter under consideration. 

What this very rudimentary exercise has demonstrated, is that we can learn something of importance simply by looking at the frequency at which a word appears in an article. In particular, if we can capture changes in the most informative set of descriptive words over time, we might be able to infer something about how the industry is changing. For instance, perhaps the importance of \emph{HIV} relative to \emph{cancer} shifted in the period between 1994 and 2004.

\section{Frequency Distributions}

Between 1991 (when the journal first started) and the end of 2004, Bioworld published 22,461 articles. Following the work of Klingenstein et al., (2014), Tirunillai and Tellis (2014) and others, I performed the following steps on each article in order to prepare the data for further analysis.
\begin{enumerate}
  \item Break apart each article into a list of lexical elements at the word level in a process typically referred to as tokenization (Jurafsky and Martin 2000). \footnote{Note for instance that compound words are split into their constituent parts so that a word like `don't' becomes three elements: `don,' `',' and `t.'}
  \item Remove all punctuation and numbers from the list.
  \item Change all elements to lower case.
  \item Remove all English `stop words' (i.e. the plumbing)--see table \ref{stop}--and words of length $< 3$.
  \item Stem the remaining elements using the Porter (1980) stemmer so that words like `work' and `working' are treated as the same lexical element.
\end{enumerate}

\begin{table}
\begin{center}
\caption[English Stopwords]{English Stopwords (i.e. the ``plumbing"). \label{stop}}
\vspace{0.3in}
\begin{tabular}{ccccc}
\hline 
\hline
i & me & my & myself & we \\
our & ours & ourselves & you & your \\
yours & yourself & yourselves & he & him \\
his & himself & she & her & hers\\
herself & it & its & itself & they\\
them & their & theirs & themselves & what \\
which & who & whom & this & that \\
these & those & am & is & are\\
was & were & be & been & being \\
have & has & had & having & do\\
does & did & doing & a & an \\
the & and & but & if & or \\
because & as & until & while & of \\
at & by & for & with & about \\
against & between & into & through & during \\
before & after & above & below & to \\
from & up & down & in & out \\
on & off & over & under & again \\
further & then & once & here & there \\
when & where & why & how & all \\
any & both & each & few & more \\
most & other & some & such & no \\
nor & not & only & own & same \\
so & than & too & very & s \\
t & can & will & just & don \\
should & now & & & \\
\hline
\end{tabular}
\end{center}
\end{table}

Following the preprocessing steps described above, the lists of lexical elements representing each article were pooled by month into 168 larger lists--one for each month between January 1991 and December 2004. \footnote{Various aggregation sizes were tested and the results (shown later) remain qualitatively the same.} From these lists, I then constructed 168 frequency distributions, with the lexical element in rank order on the x-axis and its count divided by the total number of lexical elements constituting the y-axis--that is, that element's probability given the total list of elements in that month. Tables \ref{topwords1} and \ref{topwords2} list the top 25 word stems in rank order for each of the 14 years under consideration. 

\begin{table}
\footnotesize
\begin{center}
\caption[Top Words (1991-1997)]{Top Words (1991-1997) \label{topwords1}}
\vspace{0.3in}
\begin{tabular}{ccccccc}
\hline 
\hline
1991 & 1992 & 1993 & 1994 & 1995 & 1996 & 1997 \\
said & said & compani & said & said & said & said \\
compani & compani & said & compani & compani & million & compani \\
million & right & develop & drug & million & gene & cell \\
develop & million & cell & develop & drug & drug & drug \\
share & develop & right & million & cell & compani & gene \\
right & product & health & trial & develop & cell & million \\
health & health & million & health & gene & develop & develop \\
drug & share & product & right & trial & patient & trial \\
american & american & american & cell & product & protein & patient \\
stock & consult & drug & research & patient & year & product \\
product & drug & research & product & right & product & protein \\
consult & cell & trial & gene & research & studi & research \\
reserv & reserv & patient & patient & health & research & cancer \\
market & trial & share & american & studi & right & phase \\
cell & stock & consult & share & year & trial & human \\
nasdaq & nasdaq & reserv & consult & american & diseas & diseas \\
corp & market & gene & year & share & share & year \\
research & research & clinic & reserv & phase & human & studi \\
patent & patient & presid & phase & would & cancer & clinic \\
bioworld & presid & stock & studi & diseas & health & percent \\
trial & corp & also & clinic & consult & american & use \\
percent & technolog & diseas & bioworld & cancer & phase & share \\
year & year & technolog & would & reserv & percent & also \\
presid & clinic & nasdaq & human & percent & consult & pharmaceut \\
close & patent & market & market & human & also & treatment \\
\hline
\end{tabular}
\end{center}
\end{table}
\begin{table}
\footnotesize
\begin{center}
\caption[Top Words (1998-2004)]{Top Words (1998-2004) \label{topwords2}}
\vspace{0.3in}
\begin{tabular}{ccccccc}
\hline 
\hline
1998 & 1999 & 2000 & 2001 & 2002 & 2003 & 2004 \\
said & said & said & said & said & said & said \\
gene & compani & compani & develop & compani & compani & compani \\
cell & gene & million & compani & develop & million & escap \\
compani & cell & gene & million & drug & develop & million \\
million & million & develop & cell & patient & escap & develop \\
drug & drug & cell & drug & trial & drug & patient \\
develop & develop & drug & patient & million & patient & drug \\
patient & patient & product & product & studi & bioworld & trial \\
protein & trial & protein & trial & product & product & phase \\
product & product & trial & gene & phase & trial & product \\
trial & protein & patient & research & cell & studi & bioworld \\
cancer & phase & technolog & phase & research & phase & studi \\
year & cancer & share & studi & cancer & today & today \\
human & research & phase & technolog & technolog & research & year \\
research & year & research & cancer & diseas & cancer & cancer \\
phase & studi & diseas & diseas & also & cell & research \\
diseas & human & cancer & protein & year & percent & also \\
studi & diseas & human & human & percent & year & percent \\
use & percent & studi & year & today & also & share \\
today & use & use & treatment & pharmaceut & diseas & diseas \\
technolog & technolog & year & also & would & share & pharmaceut \\
percent & share & stock & percent & bioworld & technolog & locat \\
bioworld & stock & therapeut & use & gene & would & clinic \\
market & sequenc & pharmaceut & therapeut & clinic & stock & technolog \\
treatment & pharmaceut & clinic & pharmaceut & use & pharmaceut & cell \\
\hline
\end{tabular}
\end{center}
\end{table}

While it is impossible to draw strong conclusions from a brief scan of the word stems in Tables \ref{topwords1} and \ref{topwords2}, a few trends are apparent. For example, the  term `american' is in the top 10 for the years 1991-1993 before falling in rank in the years and 1994-1996 and then off the list completely by 1997. This comports with observations about the globalization of the biotech industry during these years as noted elsewhere in the literature (e.g. Powell et al. 1999). Second, the term `cancer' doesn't enter the list until 1995 after which point it holds a steady position in the top 20 words throughout the rest of the sample. This is consistent with reports that the science of Biotech has gradually shifted from a focus on infectious diseases (e.g. viruses) to issues related to human genetics (Wolfe 2001).

Despite the above observations, there are many high-frequency terms that are either not very descriptive or their importance is not discernible without substantial knowledge of the context. Moreover, terms that exist well below the 25-word cutoff may be more informative or terms may shift in frequency together in patterns that are difficult to discern ex ante. However, we can potentially overcome these limitations with naive examination of changes in the frequency of language use as I argued earlier in this chapter. Building on the recent work of Klingenstein et al. (2014), I develop and test a measure that tracks the amount of change that occurs from one frequency distribution to the next. The details of this measure are presented in the following sections.

\section{Kullback-Leibler Divergence}

In computational linguistics, a process called topic modeling is gaining in popularity as way to extract meaning from large, unstructured textual databases (Blei 2012). In its most basic form, the process involves the naive classification of lexical `features' into buckets based on criteria such as cooccurrence within the text. For instance, the terms `player' and `court' might co-occur in many articles about basketball and thus contribute highly to that topic (Blei and Lafferty 2009).\footnote{Note that many topic models can be viewed as a form of principal component analysis given a matrix of articles by terms Blei (2012)} Such techniques have been successfully implemented across the social sciences to understand research agendas as varied as the classification of scientific knowledge (Blei and Lafferty 2007) and brand positions in a competitive market (Tirunillai and Tellis 2014).

Nonetheless, the process under investigation in the current work is dynamic, and the most common methods for naive topic modeling produce classifications that are static across time--the order of documents is irrelevant to the production of the classificatory scheme. Even the dynamic topic model recently proposed by Blei and Lafferty (2006) imposes some restrictive assumptions. For instance, the set of features upon which the dynamic model is estimated, must be defined prior to execution of the algorithm (Blei 2012). A typical procedure is to choose the total set of features as the union of the top $X$ features by frequency from each time period. However, this allows the most prominent features that exist far into the future (i.e. vocabulary) to influence topics modeled in the distant past. Moreover, the insights one gains from such models is based on how the various features change in their influence of a persistent topic--a worthwhile endeavor, but limiting in the current context where topics (categories) are assumed to emerge, merge and disappear over time (e.g. Colyvas and Powell 2008).

An alternative approach--and the one pursued here--imposes a somewhat less burdensome restriction. Rather than define the set of features using the entire (time invariant) corpus, I draw from a moving window around the period under consideration. Changes are captured as the difference between features at time $t$ and the features defined by their average over the previous $k$ periods. Thus, I can limit features to the intersection of the top $X$ by frequency in periods $t-k$ through period $t$. Moreover, the $k$-period moving average incorporates innovations gradually so that new shocks are defined against the relevant past--the period of time most likely to exist in an actor's recent memory--rather than the field's entire history. 

The features I consider for my model are word-stems. They are mapped to a probability distribution based on the frequency with which they occur in a given time period. This is sometimes referred to as a `bag-of-words' model because it employs word-stems and their frequencies without consideration of their contextual ordering in sentences, paragraphs, etc (Jurafsky and Martin 2000). I compute the uniqueness of the distribution at period $t$ by means of the Kullback-Leibler divergence (KLD) from the average of the prior $k$ distributions (see e.g Klingenstein et al., 2014). The KLD is denoted $D_{KL}(P\|Q)$ where $P$ is assumed to represent the ``true" distribution, which in this case, is the distribution constructed from the prior $k$ periods. Q is the distribution at time $t$. For discrete distributions (as I have here) the measure is defined as,

\begin{equation}
	D_{KL}(P_k\|Q_t) = \sum_i{P_k(i)ln\frac{P_k(i)}{Q_t(i)}},
\end{equation}

\noindent which describes the logarithmic difference between the probabilities $P$ and $Q$. The consistency of this measure over time relies on the fact that the shape of the frequency distribution doesn't change appreciably with changes in the text--a fact grounded in Zipf's law (Zipf 1932; Adamic and Guberman 2002). Thus changes are largely based on a reordering of features within the distribution rather than a reconfiguration of it's shape. Figures \ref{zipf1} and \ref{zipf2} illustrate this fact graphically by showing the the same power law relationship between words and their frequencies despite different orderings and higher frequencies in general for the later time period.

\begin{figure}
\begin{center}
\includegraphics[scale=.4]{../figures/zipf1.png}
\caption[Frequency Distribution 1991]{Frequency Distribution for year 1991 top 50 word stems. \label{zipf1}}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=.4]{../figures/zipf2.png}
\caption[Frequency Distribution 2001]{Frequency Distribution for year 2001 top 50 word stems. \label{zipf2}}
\end{center}
\end{figure}

The KLD quantifies the amount of information lost when $Q$ is used to predict $P$. As such, it captures the new information represented by the current set of word-stem frequencies. When the value is high, it describes a departure from the prior $k$ distributions. However, when the value is low, it represents consistency with prior word-stem frequencies. Because the KLD always takes on values between 0 and 1, I can define a measure of language consistency over time as,

\begin{equation}
	LCON_t=1-D_{KL}(P_k\|Q_t).
\end{equation}


\section{Descriptive Statistics}

For the descriptive statistics and analysis that follow, my measure of language consistency is based on probability distributions derived from the intersection of the top 1,000 word-stems in the current month ($Q_t$) and the prior three ($P_k$). Several other configurations were tested as as part of my robustness checks and the results are qualitatively the same. Table \ref{freq} shows the average monthly word stems, vocabulary (i.e. unique word-stems) and consistency over the fourteen year period under investigation.

Several trends are immediately apparent. First, the amount of writing grew substantially throughout the 1990's before leveling off around year 2000. Second, the vocabulary used to describe the industry more than doubled over the same period with the sharpest increase occurring between 1992 and 1993. Third, the consistency in the way the language was used rose gradually over the same period. If, as I argued previously, the frequency of language use is indicative of the legitimation process (e.g. Pektrova et al. 2014; Rosa et al 1999), then this rise in consistency suggests that the industry as a whole has undergone a gradual march towards the establishment of greater field-wide knowledge and shared understanding (Cattani et al., 2008). That is, the consistency observed in language may proxy for shared understanding, or consensus, at the field level.

\begin{table}
\begin{center}
\caption[Language Frequencies by Year]{Language Frequencies by Year \label{freq}}
\vspace{0.3in}
\begin{tabular}{cccc}
\hline 
\hline
YEAR & WORDS & VOCABULARY & LCON \\
\hline
1991 & 57,956 & 4816 & .8783283 \\
1992	 & 147,557 & 7,478 & .9098001 \\
1993 &  207,807 & 10,019 & .9397479 \\
1994 &  234,350 & 10,743 & .9359598 \\
1995 &  215,690 & 9,680 & .9437492 \\
1996 &  214,447 & 9,854 & .9425526 \\
1997 &  238,250 & 10,594 & .9438075 \\
1998 &  243,287 & 11,240 & .9512883 \\
1999 &  267,435 & 12,593 & .9478587 \\
2000 &  328,733 & 13,484 & .9557859 \\
2001 &  329,704 & 14,673 & .9540956 \\
2002 &  342,010 & 13,854 & .9625357 \\
2003 &  374,304 & 13,286 & .9642392 \\
2004 &  350,575 & 12,525 & .9612578 \\
\hline
\end{tabular}
\end{center}
\end{table}



However, despite this gradual increase in consistency year over year, there is significant variation at the monthly level. Figure \ref{lcon1} shows the language consistency measure plotted on a monthly basis from 1991 through 2004. Notably, there is significant volatility in the first couple of years, which may be due to institutional factors unrelated to the legitimation process. For instance, new reporters must be recruited and learn how to write about the industry. As mentioned above, the vocabulary more than doubled in these first few years. There also appears to be a periodic set of peaks suggesting some seasonal regularity in the writing.

%\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[scale=.4]{../figures/lcon1.png}
\caption[Monthly Language Consistency (1991-2004)]{Monthly Language Consistency (1991-2004)\label{lcon1}}
\end{center}
\end{figure}
%\end{landscape}

To account for possible seasonal biases I recalculate the measure using the residuals from a regression on monthly fixed effects. Figure \ref{lcon2} removes the first two years of data and shows the language consistency measure for the years 1993 onwards in both levels (top) and first differences (bottom). The seasonal bias appears to be resolved; however, several additional features are worth noting. There are several downward spikes in the upper graph, which represent strong deviations from the prior three months of language use. In several cases, this is followed by a rise back to relative consistency suggesting that the change in language was incorporated and adopted. In other words, some industry shift took place, which then became a permanent feature of the lexicon.

One notable exception are the years 1999 through 2001 during which the consistency of the language fluctuates several times. This is most apparent when looking at the (lower) graph in first differences around this time. Clearly the the volatility of this measure is pronounced. This coincides with the general turmoil surrounding the dotcom bubble and the subsequent spillover into other industries. For instance, many biotech firms went out of business during this period for lack of financing while other sought the safety of alliances with large pharmaceutical companies (Wolfe 2001).

%\begin{landscape}
\begin{figure}
%\begin{figure}[p!]
\begin{center}
\includegraphics[scale=.4]{../figures/lcon2.png}
\caption[Monthly Language Consistency, Levels and Differences (1993-2004)]{Monthly Language Consistency in Levels and First Differences (1993-2004)\label{lcon2}}
\end{center}
\end{figure}
%\end{landscape}

Figure \ref{lcon3} highlights the most significant drops in consistency over the 12 year period (1993-2004). If these drops truly represent paradigm shifts, then we would expect the differences in language in the months just following to be significantly different from the months just prior. The null hypothesis in this case is that the drop was simply a temporary deviation and the language reverted back to its prior distribution. This would occur with a double-deviation, where the distribution deviates once from the original distribution and then deviates back again to the original. In this case, the distributions prior to and following the deviation, should be relatively similar.

%\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[scale=.4]{../figures/lcon3.png}
\caption[Highlighted Deviations in Monthly Language Consistency]{Highlighted Deviations in Monthly Language Consistency\label{lcon3}}
\end{center}
\end{figure}
%\end{landscape}

To examine the nature of the deviations which occurred in March 1995, December 1996 and July 2001, I calculated the rank correlation of the word-stems (by frequency) between the target date $t$ and language from dates 3 months post ($t+3$) and prior ($t-3$) to the target. If the deviation represents a permanent shift in language use, then the rank correlation between $t$ and $t+3$ should be higher than the correlation between $t$ and $t-3$. Moreover, the correlation between $t+3$ and $t-3$ should be similar to that of $t$ and $t-3$, indicating that the language of $t$ and $t+3$ are indeed different from $t-3$ in the same way.

Table \ref{shift} shows these rank correlations for the three highlighted dates. In all three cases, language from the target date is more similar to subsequent language than it is to prior language. Moreover, the correlations between language at time $t-3$ with language at both $t$ and $t+3$ is very similar, suggesting that the changes that occurred during the deviation persisted into the future. The correlations with future language are highest for the deviations in 1995 and 2001 and somewhat smaller for 1996. The deviation in 1996 was the smallest of the three to begin with and fewer of the changes in lexicon may have been incorporated going forward. 
 
\begin{table}
\begin{center}
\caption[Rank Correlations of Language after Large Deviations]{Rank Correlations of Language after Large Deviations \label{shift}}
\vspace{0.3in}
\begin{tabular}{cccc}
\hline 
\hline
DATE & $t$ corr. $t-3$ & $t+3$ corr. $t-3$ & $t$ corr. $t+3$ \\
\hline
04/1995 & 0.717 & 0.710 & 0.772 \\
12/1996 & 0.740 & 0.747 & 0.756 \\
07/2001 & 0.791 & 0.798 & 0.841 \\
\hline
\end{tabular}
\end{center}
\end{table}
 
Figure 





\section{Consensus and the Stock Market}
\begin{enumerate}
  \item Show patterns with trading volume
  \item Show statistical analysis of relation to trading volume
  \item However, confounded with macro economic changes
  \item However, confounded with relationship to systematic risk
  \item Show short term reactions and interdependencies.
  \item However, there may be significant heterogeneity across firms
  \item This will be handled in next section
 \end{enumerate}


