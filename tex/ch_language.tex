\chapter{Measuring Uncertainty Using Language\label{lang}}

\section{Introduction}
Chapter \ref{law} introduced an information theoretic definition of knowledge heterogeneity based on the concept of entropy. As noted in section 2.4, it can be tempting to conflate high levels of market entropy with the concept of uncertainty. This stems from the mathematical definition of high entropy as a state with little information (i.e. high disorder). However, information (under the proposed definition) refers to the \emph{differences} between market actors. When we can distinguish one actor from the next, we have some information about the market. When we can't, we have no information. Thus, it is meaningful differences (not similarity) that are the source of mathematical order in the market entropy construct. 

Nonetheless, knowledge heterogeneity may describe a particular type of environmental uncertainty. For instance, the uncertainty that results from disagreement over the importance (or appropriateness) of market behaviors (practices, products, etc.). As alluded to in the introduction, this will naturally occur during transitions of technology, taste and attention if innovations are assumed to arise locally and diffuse gradually (Colyvas 2007; Rogers 2002). For a period following the innovation, the variance in knowledge and opinion of that innovation will be more pronounced.

In the sections that follow I review the literature on environmental uncertainty and its measurement. Drawing on social constructionist theories and insights from research on the legitimation process (Hannan and Carroll 1992; Rosa et al., 1999), I then propose a new measure based on changes in language use over time. Finally, I show how this measure is related to exchange behavior in the biotechnology industry.

\section{Environmental Uncertainty, Legitimacy and Language}
\begin{small}
\begin{quote}
``Grammars in both cooking and engineering exist not just as rules but as a set of unspoken practices taken for granted" (Arthur 2009 p. 77)
\end{quote}
\end{small}

Capturing the effects of environmental uncertainty over time is no easy task. Indeed, much of the noteworthy empirical work in this area defines uncertainty according to some exogenously determined classification, and often one that is fixed in time. For instance, Podolny (1994) examines exchange relationships in the issuance of investment-grade vs. non-investment-grade debt. Firms operating in the latter context are assumed to experience higher levels of uncertainty. In a more contemporary rendering, Erdem et al. (2006) use a country's predetermined score on an uncertainty avoidance scale to make claims about how individuals in that country react to signals of brand credibility. 

Studies that attempt to track environmental uncertainty over time, often do so using measures of volatility in a portfolio of stocks (see e.g. Beckman et al. 2004). However, this might be better understood as systematic risk (Thomaz and Swaminathan 2014). While it may be a consequence of uncertainty, risk is theoretically distinct; under risk, the probabilities associated with future outcomes are assumed to be known, whereas uncertain prospects have unknown probabilities (Tversky and Fox 1995). More importantly, the stock market represents a limited sample of market participants and likely misses important behavior undertaken by smaller or newer participants in a field. 

Other studies have looked at the percentage change in an industry's patenting activity as a way to capture technical uncertainty (see e.g. Goerzen 2007; Luque 2002). However, technical uncertainty is only one dimension of environmental uncertainty and patents are only one facet of market behavior. Moreover, patenting activity is itself subject to changes in practice over time. For example, biotech firms have learned to over-apply for patents as a way to conceal the innovations they are actually using (Wolfe 2001).

A parallel stream of research has examined the related concept of legitimacy, which can be defined as ``consensus among agents (audiences) that the features and activities of [market actors] are appropriate and desirable within a widespread, taken-for-granted system of norms or social codes" (Cattani et al. 2008, p. 147). Legitimacy is linked to uncertainty through the legitimation process, which typically occurs over a long period of time and involves many actors. The process eventually leads to the establishment of field-wide knowledge and shared understanding of the boundaries and definition of relevant technological dimensions (Bijker, Hughes, and Pinch, 1987; Santos and Eisenhardt, 2009), the meaning of new product categories (Kennedy, Lo, and Lounsbury, 2010; Rosa et al., 1999), and the validity of new practices (Lounsbury and Crumley, 2007; Tripsas, 2009). However, as the legitimation process unfolds, there are periods of high disagreement (e.g. Colyvas and Powell 2008), which increase the uncertainty actors have about market reactions to their behavior (practices, products etc.).

This formulation of environmental uncertainty--derived from consideration of the legitimation process--comports nicely with my focus on field-level transitions in technology, taste and attention. However, a brief review of the empirical literature on legitimacy raises an additional concern: the behavior deemed legitimate is typically defined ex post--that is, operationalizations are based on the present incarnation of a category and then traced to some earlier period. For example, Petkova et al., (2014) examine the legitimacy of investing in the clean energy sector by counting historical references to the terms ``clean energy," ``green energy" and ``alternative energy" in media articles. In practice, categories emerge organically (Goldberg 2013) and may cycle through several incarnations of appropriate behavior before they settle on the perspicuously demarcated boundaries that appear to the present observer. More importantly, this formulation excludes the struggle between competing paradigms that characterizes the typical legitimation process (Colyvas and Powell 2008).

How then, do we attempt to understand the uncertainty caused by transitions in technology, taste and attention in an unbiased manner? Despite it's shortcomings, the literature on legitimacy provides several useful insights. Foremost, the frequency of  behavior is a meaningful yardstick of general acceptance (Hannan and Carroll, 1992). Second, the frequencies of language used in the media are representative of an unfolding legitimation process (Petkova et al., 2014; Rosa et al., 1999). Using these insights, I propose a general notion of environmental uncertainty based on changes in the frequency of language use. This can be accomplished without imposing ex post classifications by tracking language frequencies as they naturally occur--that is, the frequency distribution of descriptive words at various points in time. Changes in a frequency distribution from one period to the next is thus assumed to represent shifts in the importance of behavior described by the language. The magnitude and volatility of these changes can capture disagreement about importance and hence uncertainty as previously described. 

\section{Methodology}

A variety of recent studies have started to look at the generic characteristics of written language as a way to understand the structure of markets and the evolution of human behavior (Klingenstein, Hitchcock, and DeDeo 2014; Goldberg 2013; Tirunillai and Tellis 2014). The viability of this type of research is driven by parallel developments in the availability of digital archives and methods appropriate for the analysis of ``big data." While the standards and practices used for large-scale textual analysis are still very much under development, I have largely followed the work of prior scholars in preparing my data and constructing my measures. The following paragraphs describe this process in detail.

Textual data is sourced from articles written for the Bioworld trade journal between the years 1991 and 2004. During this period, Bioworld constituted a primary source for industry-wide dissemination of information about the activities of firms and other stakeholders in the biotechnology industry. They published daily articles (excluding weekends) on topics as wide-ranging as personnel changes, fundraising activity, inter-firm contracting, the FDA approval process and the latest scientific trends. Below are two different example articles--the first from 1994 and the second from 2004--that illustrate the level of detail typically present in the text.

\begin{singlespace}
\begin{small}
\begin{quotation}
\noindent \textbf{Teaching Cells To Make Their Own Anti-Aids Interferon} \\

It's one thing to administer recombinant interferon as antiviral therapy. Educating cells to make their own do-it-yourself interferon is something else. A team of French scientists reports doing just that to hamstring the AIDS virus. Their paper in the current Proceedings of the National Academy of Sciences (PNAS) describes: ``Blocking of retroviral infection at a step prior to reverse transcription in cells transformed to constitutively express interferon b." The group's leader, Edward DeMaeyer of France's National Scientific Research Institute (INRA) stated, ``We are developing methods for somatic-cell gene therapy directed against infection with human immunodeficiency virus ...through the constitutive production of autocrine interferon (IFN)." His strategy aims to block HIV at its earliest stages of cell entry and replication, rather than the later stages at which interferon is commonly thought to act.

For starters, the team transfected various mouse and human cell lines with IFN genes. These transformed cells synthesized interferon at a constant low rate; their replication and survival were the same as in control cells. When challenged with various retroviruses, they markedly reduced the number of virally infected cells. Then the team assailed CEM cells with HIV viral particles. (CEM is a model human T-cell line derived from patients with leukemia.) ``Two days later," as DeMaeyer reported, ``the presence of HIV-1 proviral DNA was significantly lower in the human IFN-b transformed cells than in the control cells, indicating an early block of the infectious cycle."

Six hours after the onset of infection, 40 to 80 percent of the viral particles could still be found in the transformed cells' culture medium, whereas 90 percent of the control cells' virions had moved on to infect other cells. But the transformed CEM cells died within a week or so. ``These findings," DeMaeyer concluded, are encouraging for the use of the ...IFN-b vector to explore the possibility of developing an anti-HIV-b-directed somatic cell gene therapy." Richard Mulligan at MIT's Whitehead Institute of Biomedical Research supplied the vector plasmids to the French investigators.

SInce submitting his findings to PNAS last September, DeMaeyer has determined that virus-resistant, inferferon-synthesizing CEM cells ``survive for as long as 80 days. Maybe they lived even longer," he told BioWorld in a telephone interview, ``but that's when we stopped the experiment." Moreover, moving from off-the-shelf CEM cell lines, he and his group have shown ``that the same effect is obtained in fresh peripheral blood lymphocytes taken from people. Of course," he added, "this is more
significant in terms of somatic gene therapy. That's the cell that one wants eventually to protect in the body."

The next step, he said, is to proceed from in vitro to in vivo testing. ``We want to collaborate with a group that has the rhesus monkey model going, infectable with the simian immune deficiency virus, SIV." He is already in discussion with one such research center, but is still open to other offers.

DeMaeyer cited three goals of such testing: First, to show that it works in vivo; second, to determine that the transformed cell's
immunological function is not impaired; third, to solve the problem of ``getting enough of those interferon-transformed, virus-resistant cells into the body of a seropositive person infected with AIDS."\\

\noindent \textbf{Threshold Pharmaceuticals Inc. began patient enrollment in a Phase III trial of glufosfamide for pancreatic cancer.} \\

The pivotal study is designed to test the company's lead product in patients with metastatic pancreatic cancer refractory to first-line treatment. Its primary endpoint will compare the median survival of patients treated with glufosfamide and best supportive care to those solely receiving best supportive care - all medical or surgical interventions that a pancreatic cancer patient should receive to palliate the cancer but excluding treatment with systemic therapies intended to kill the cancer cells.

At present, there are no approved therapies for those patients, who generally have an expected survival of about three months. ``In fact, one would really say there are no proven options for systemic treatment of their cancer," Threshold President George Tidmarsh told \emph{BioWorld Today}. Of the 31,860 patients expected to be diagnosed with pancreatic cancer in the U.S. this year, about 31,270 will die from the disease, according to American Cancer Society statistics.

The South San Francisco-based company agreed with the FDA on a special protocol assessment, which indicates that if the trial successfully meets its primary endpoint, the data will provide support for an efficacy claim in an eventual marketing application. Tidmarsh said the FDA requested only a few minor wording changes to Threshold's study design, case-reporting system and statistical analyses plans as both parties came to an agreement in the process.

Enrollment is under way in the U.S., while more sites will be added in Eastern Europe and South America. Eventually the trial will involve about 306 patients; recruitment is expected to last between 12 months and 15 months. Eligible patients will be randomized to receive glufosfamide every three weeks in addition to best supportive care, or best supportive care alone.

``There is a planned analysis on an interim data set that we project to have in about 15 months," Tidmarsh said, noting that the timeline would depend on enrollment. ``Our anticipation is that the complete data, with all finalized study reports, would be available in approximately 18 to 24 months."

Secondary endpoints in the trial include disease-free survival and time to disease progression, as well as response and safety evaluations. A next-generation chemotherapeutic agent from the alkylator class, it is designed to enter cells through up-regulated glucose transport proteins.

``The toxin is attached to the glucose molecule, so we believe that it allows the drug to target specifically to the cancer through a mechanism we call metabolic targeting," Tidmarsh said. ``And metabolic targeting is based on the dramatic increased consumption of glucose that tumor cells have relative to normal cells."

He added that glufosfamide produces fewer side effects than prior-generation alkylators, which generated toxins that led to hemorrhagic cystitis. Prior studies have provided a glimpse at the drug's potential efficacy in fighting pancreatic cancer.

Data from a Phase II trial, reported at the America Association for Cancer Research meeting in 2002 and published in the November 2003 issue of the \emph{European Journal of Cancer}, revealed objective tumor shrinkage. An updated analysis of the survival shows an estimated 9 percent two-year survival, which the company said compares to 1 percent or less in historical studies with other first-line therapies. The fully enrolled study included 35 chemotherapyna ve patients with locally advanced or metastatic pancreatic cancer.  

Findings from an initial Phase I trial, published in the October 2000 edition of the \emph{Journal of Clinical Oncology}, showed that its only pancreatic cancer participant achieved a complete remission of disease and remained in remission more than five years later after receiving glufosfamide treatment alone.

Threshold owns all rights to the compound, and Tidmarsh said the company would continue to steer development on its own. It also has demonstrated glufosfamide's activity in Phase II studies in breast and colon cancers, findings which were reported at this year's American Society of Clinical Oncology meeting, and plans to explore its use in lymphoma.

Outside of glufosfamide's development, Threshold is developing two other clinical-stage products, both of which also are based on the company's metabolic targeting approach. A compound labeled TH-070 is in Phase II for benign prostatic hyperplasia, while its 2-deoxyglucose product is in Phase I for solid tumors.

Earlier this year, Threshold filed to go public, though its initial public offering has yet to price. (See \emph{BioWorld Today}, April 15, 2004.) 
\end{quotation}
\end{small}
\end{singlespace}

With even a quick glance at the prior two articles, one can notice several features of the text. First, there are a lot of industry-specific words dealing with the science of biotechnology--words like `glufosfamide' and `virion.' These words are very specific and exist at a very low frequency in the text. As such, they provide very little insight into the broad subject matter of the article. In contrast, words like `the,' `to,' `is,' `of,' and `was' show up everywhere. These words constitute the ``plumbing" of the english language and are also uninformative.

Between these two extremes however, are words like `HIV,' `cancer' and `cell.' These words are descriptive, but not overly specific. Moreover, they appear with sufficient periodicity to suggest they are an integral part of subject matter under consideration. What this very rudimentary exercise has demonstrated is that we may be able to learn some thing of importance about these articles simply by looking at the frequency distribution of the words that appear in them. In particular, if we can capture changes in the most informative set of descriptive words over time, we might be able to infer something about how the industry is changing. For instance, perhaps the importance of \emph{HIV} relative to \emph{cancer} shifted in the period between 1994 and 2004.

\section{Frequency Distributions}

Between April of 1991 (when the journal first started) and 2004, Bioworld published more than 22,000 articles. Following the work of Klingenstein et al., 2014, Tirunillai and Tellis (2014) and others, I performed the following steps on each article in order to prepare the data for further analysis.
\begin{enumerate}
  \item All 
  \item 
  \item 
\end{enumerate}


Frequency distributions were then constructed. 

Table \ref{frequencies} shows the 

Show change in top descriptive words over time

\section{Kullback-Leibler Divergence}

\begin{enumerate}
  \item Show how entropy divergence has dropped over time
  \item Show words associated with large jumps
\end{enumerate}

\section{Entropy and Exchange}
\begin{enumerate}
  \item Show patterns with trading volume
  \item Show statistical analysis of relation to trading volume
  \item However, confounded with macro economic changes
  \item However, confounded with relationship to systematic risk
  \item Show short term reactions and interdependencies.
  \item However, there may be significant heterogeneity across firms
  \item This will be handled in next section
 \end{enumerate}


