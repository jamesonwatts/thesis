\chapter{Legitimacy and Language\label{lang}}

\begin{small}
\begin{quote}
``Grammars in both cooking and engineering exist not just as rules but as a set of unspoken practices taken for granted" \citep[p. 77]{arthur2009}.
\end{quote}
\end{small}
Chapter \ref{law} introduced a concept called `market entropy' as a way to describe the degree to which actors are similar or different in their representations of the market. This is a broad concept which incorporates insights from several established literatures published under the neoinstitutionalist moniker \citep[e.g.][]{denzau1994, dimaggio1983, loasby1999}. In this chapter, I narrow the scope of investigation slightly to focus on one of these literatures; namely, research on legitimacy and the legitimation process. This focus allows me to pay greater attention to the mechanisms that influence the distribution of knowledge during transitions of technology, taste and attention \citep[e.g.][]{powell2008}.

Legitimacy can be defined as ``consensus among agents (audiences) that the features and activities of [market actors] are appropriate and desirable within a widespread, taken-for-granted system of norms or social codes" \citep[p. 147]{cattani2008}. Legitimacy is distinct from the more general `market entropy' construct in that it imposes a moral dimension on shared understanding--that is, behavior is not just `taken-for-granted' but also `appropriate.' Nonetheless, a substantial body of research on the legitimation \emph{process} can help us understand how shared understandings actually come about in practice \citep{goldberg2012}.

Typically, legitimation occurs over a long period of time and involves many actors \citep{goldberg2012, rosa1999}. The process eventually leads to the establishment of field-wide knowledge and shared understanding of the boundaries and definition of relevant technological dimensions \citep{santos2009}, the meaning of new product categories \citep{rosa1999}, and the validity of new practices \citep{lounsbury2007, tripsas2009}. 

However, as a legitimation process unfolds, there are often periods of high disagreement \citep[e.g.][]{powell2008}. During these periods, actor's representations of the market are either in conflict or not sufficiently diffuse. Through a process of negotiation and diffusion, dominant understandings (designs, practices, products, etc.) emerge as the `taken-for-granted' within a field \citep{cattani2008}. This is the framework within which subsequent developments are then judged.

This formulation of shared understanding--derived from consideration of the legitimation process--comports nicely with my focus on field-level transitions in technology, taste and attention. However, a brief review of the empirical literature on legitimacy raises an additional concern: the behavior deemed legitimate is typically defined ex post--that is, operationalizations are based on the present incarnation of a legitimized category and then traced to some earlier period. For example, \citet{petkova2014} examine the legitimacy of investing in the clean energy sector by counting historical references to the terms ``clean energy," ``green energy" and ``alternative energy" in media articles. In practice, categories emerge organically \citep{goldberg2012} and may cycle through several incarnations of appropriate behavior before they settle on the perspicuously demarcated boundaries that appear to the present observer. More importantly, this formulation excludes the struggle between competing paradigms that characterizes the typical legitimation process \citep{colyvas2007, powell2008}.

How then, do we attempt to understand transitions in technology, taste and attention in an unbiased manner? Despite certain shortcomings, the literature on legitimacy provides several useful insights. Foremost, the frequency of  behavior is a meaningful yardstick of general acceptance \citep{hannan1992}. Second, the frequencies of language used in the media are representative of an unfolding legitimation process \citep{petkova2014, rosa1999}. Using these insights, I propose a general notion of shared understanding based on changes in the frequency of language use. This can be accomplished without imposing ex post classifications by tracking language frequencies as they naturally occur--that is, the frequency distribution of descriptive words at various points in time. Changes in a frequency distribution from one period to the next is thus assumed to represent shifts in the importance of behavior described by the language. In contrast, similar distributions from one period to the next may represent an emerging consensus in the field. 

\section{Language Models}

A variety of recent studies have started to look at the generic characteristics of written language as a way to understand the structure of markets and the evolution of human behavior \citep{klingenstein2014, goldberg2012, tirunillai2014}. The viability of this type of research is driven by parallel developments in the availability of digital archives and methods appropriate for the analysis of ``big data." While the standards and practices used for large-scale textual analysis are still very much under development, I have largely followed the work of prior scholars in preparing my data and constructing my measures. The following paragraphs describe this process in detail.

Textual data is sourced from articles written for the Bioworld trade journal between January 1991 and January 2004. During this period, Bioworld constituted a primary source for industry-wide dissemination of information about the activities of firms and other stakeholders in the biotechnology industry \citep{wolff2001}. They published several articles each day (excluding weekends) on topics as wide-ranging as personnel changes, fundraising activity, inter-firm contracting, the FDA approval process and the latest scientific trends. Below are excerpts from two different example articles. The first is from Tuesday, April 5, 1994 and the second is from Monday, December 1, 2003. They were selected to illustrate the level of detail typically present in the text.

\begin{singlespace}
\begin{small}
\begin{quotation}
\noindent \textbf{Teaching Cells To Make Their Own Anti-Aids Interferon} \\

``...to reverse transcription in cells transformed to constitutively express interferon b." The group's leader, Edward DeMaeyer of France's National Scientific Research Institute (INRA) stated, ``We are developing methods for somatic-cell gene therapy directed against infection with human immunodeficiency virus ...through the constitutive production of autocrine interferon (IFN)." His strategy aims to block HIV at its earliest stages of cell entry and replication, rather than the later stages at which interferon is commonly thought to act.

For starters, the team transfected various mouse and human cell lines with IFN genes. These transformed cells synthesized interferon at a constant low rate; their replication and survival were the same as in control cells. When challenged with various retroviruses, they markedly reduced the number of virally infected cells. Then the team assailed CEM cells with HIV viral particles. (CEM is a model human T-cell line derived from patients with leukemia.) ``Two days later," as DeMaeyer reported, ``the presence of HIV-1 proviral DNA was significantly lower in the human IFN-b transformed cells...\\

\noindent \textbf{Praecis Gets First Approval: Plenaxis In Prostate Cancer} \\

...gonadotropin-releasing hormone antagonist available as a depot formulation, Plenaxis (abarelix for injectable suspension) is indicated for pain relief in advanced symptomatic prostate cancer patients who meet a variety of conditions. First, they are not viable candidates for treatment with luteinizing hormone-releasing hormone agonists, and they refuse surgical castration. Patients must exhibit one or more other conditions, including a risk of neurological compromise due to metastases, ureteral or bladder outlet obstruction due to local encroachment or metastatic disease, or severe bone pain from skeletal metastases persisting on narcotic analgesia.

``Most importantly, this product is totally homegrown," Praecis Chairman and CEO Malcolm Gefter told \emph{BioWorld Today}. ``Its concept and inception were conceived at the founding of the company, and we have executed every single aspect of the research, development, formulation, regulatory [filings], manufacturing and now sales and marketing all by ourselves. It really represents our ability to execute a fully integrated business strategy..."

\end{quotation}
\end{small}
\end{singlespace}

With even a quick glance at the prior two articles, one will notice several prominent features. First, there are a large number of industry-specific words dealing with the science of biotechnology--words like `luteinizing' and `virion.' These words are very specific and exist at a very low frequency in the text. Because of this, they provide little insight into the broad subject matter of the article. In contrast, words like `the,' `to,' `is,' `of,' and `was' show up everywhere. However, these words constitute the ``plumbing" of the english language and are also uninformative. Between these two extremes, are words like `HIV,' `cancer' and `cell.' These words are descriptive, but not overly specific. Moreover, they appear with sufficient periodicity to suggest they are an integral part of subject matter under consideration. If I can capture changes in how these descriptive words are used over time, I might be able to infer something about how the industry is changing. For instance, perhaps the importance of \emph{HIV} relative to \emph{cancer} shifted in the period between 1994 and 2003.

\section{Frequency Distributions}

Between 1991 (when the journal first started) and the end of 2003, Bioworld published 20,999 articles. Following the work of \citet{klingenstein2014}, \citet{tirunillai2014} and others, I performed the following steps on each article in order to prepare the data for further analysis.
\begin{small}
\begin{enumerate}
  \item Break apart each article into a list of lexical elements at the word level in a process typically referred to as tokenization \citep{jurafsky2000}. \footnote{Note for instance that compound words are split into their constituent parts so that a word like `don't' becomes three elements: `don,' `',' and `t.'}
  \item Remove all punctuation and numbers from the list.
  \item Change all elements to lower case.
  \item Remove all English `stop words' (i.e. the plumbing)--see table \ref{stop}--and words of length $< 3$.
  \item Stem the remaining elements using the \citet{porter1980} stemmer so that words like `work' and `working' are treated as the same lexical element.
\end{enumerate}
\end{small}

\begin{table}
\begin{center}
\caption[English Stopwords]{English Stopwords (i.e. the ``plumbing"). \label{stop}}
\vspace{0.3in}
\begin{tabular}{ccccc}
\hline 
\hline
i & me & my & myself & we \\
our & ours & ourselves & you & your \\
yours & yourself & yourselves & he & him \\
his & himself & she & her & hers\\
herself & it & its & itself & they\\
them & their & theirs & themselves & what \\
which & who & whom & this & that \\
these & those & am & is & are\\
was & were & be & been & being \\
have & has & had & having & do\\
does & did & doing & a & an \\
the & and & but & if & or \\
because & as & until & while & of \\
at & by & for & with & about \\
against & between & into & through & during \\
before & after & above & below & to \\
from & up & down & in & out \\
on & off & over & under & again \\
further & then & once & here & there \\
when & where & why & how & all \\
any & both & each & few & more \\
most & other & some & such & no \\
nor & not & only & own & same \\
so & than & too & very & s \\
t & can & will & just & don \\
should & now & & & \\
\hline
\end{tabular}
\end{center}
\end{table}

Following the preprocessing steps described above, the lists of lexical elements representing each article were pooled by month into 156 larger lists--one for each month from January 1991 through December 2003. \footnote{Various aggregation sizes were tested and the results (in Appendix \ref{apndxA}) remain qualitatively the same.} From these lists, I then constructed 156 frequency distributions, with the lexical element in rank order on the x-axis and its count divided by the total number of lexical elements constituting the y-axis--that is, that element's probability given the total list of elements in that month. Tables \ref{topwords1} and \ref{topwords2} list the top 25 word-stems in rank order for each of the 13 years under consideration. 

\begin{table}
\footnotesize
\begin{center}
\caption[Top Words (1991-1997)]{Top Words (1991-1997) \label{topwords1}}
\vspace{0.3in}
\begin{tabular}{ccccccc}
\hline 
\hline
1991 & 1992 & 1993 & 1994 & 1995 & 1996 & 1997 \\
said & said & compani & said & said & said & said \\
compani & compani & said & compani & compani & million & compani \\
million & right & develop & drug & million & gene & cell \\
develop & million & cell & develop & drug & drug & drug \\
share & develop & right & million & cell & compani & gene \\
right & product & health & trial & develop & cell & million \\
health & health & million & health & gene & develop & develop \\
drug & share & product & right & trial & patient & trial \\
american & american & american & cell & product & protein & patient \\
stock & consult & drug & research & patient & year & product \\
product & drug & research & product & right & product & protein \\
consult & cell & trial & gene & research & studi & research \\
reserv & reserv & patient & patient & health & research & cancer \\
market & trial & share & american & studi & right & phase \\
cell & stock & consult & share & year & trial & human \\
nasdaq & nasdaq & reserv & consult & american & diseas & diseas \\
corp & market & gene & year & share & share & year \\
research & research & clinic & reserv & phase & human & studi \\
patent & patient & presid & phase & would & cancer & clinic \\
bioworld & presid & stock & studi & diseas & health & percent \\
trial & corp & also & clinic & consult & american & use \\
percent & technolog & diseas & bioworld & cancer & phase & share \\
year & year & technolog & would & reserv & percent & also \\
presid & clinic & nasdaq & human & percent & consult & pharmaceut \\
close & patent & market & market & human & also & treatment \\
\hline
\end{tabular}
\end{center}
\end{table}
\begin{table}
\footnotesize
\begin{center}
\caption[Top Words (1998-2003)]{Top Words (1998-2003) \label{topwords2}}
\vspace{0.3in}
\begin{tabular}{ccccccc}
\hline 
\hline
1998 & 1999 & 2000 & 2001 & 2002 & 2003  \\
said & said & said & said & said & said  \\
gene & compani & compani & develop & compani & compani  \\
cell & gene & million & compani & develop & million  \\
compani & cell & gene & million & drug & develop  \\
million & million & develop & cell & patient & escap  \\
drug & drug & cell & drug & trial & drug  \\
develop & develop & drug & patient & million & patient  \\
patient & patient & product & product & studi & bioworld  \\
protein & trial & protein & trial & product & product  \\
product & product & trial & gene & phase & trial  \\
trial & protein & patient & research & cell & studi  \\
cancer & phase & technolog & phase & research & phase  \\
year & cancer & share & studi & cancer & today  \\
human & research & phase & technolog & technolog & research  \\
research & year & research & cancer & diseas & cancer  \\
phase & studi & diseas & diseas & also & cell  \\
diseas & human & cancer & protein & year & percent  \\
studi & diseas & human & human & percent & year  \\
use & percent & studi & year & today & also  \\
today & use & use & treatment & pharmaceut & diseas  \\
technolog & technolog & year & also & would & share  \\
percent & share & stock & percent & bioworld & technolog  \\
bioworld & stock & therapeut & use & gene & would  \\
market & sequenc & pharmaceut & therapeut & clinic & stock \\
treatment & pharmaceut & clinic & pharmaceut & use & pharmaceut  \\
\hline
\end{tabular}
\end{center}
\end{table}

While it is impossible to draw strong conclusions from a brief scan of the word-stems in Tables \ref{topwords1} and \ref{topwords2}, a few trends are apparent. For example, the  term `american' is in the top 10 for the years 1991-1993 before falling in rank in the years and 1994-1996 and then off the list completely by 1997. This comports with observations about the globalization of the biotech industry during these years as noted elsewhere in the literature \citep[e.g.][]{powell1999}. Second, the term `cancer' doesn't enter the list until 1995 after which point it holds a steady position in the top 20 words throughout the rest of the sample. This is consistent with reports that the science of Biotech has gradually shifted from a focus on infectious diseases (e.g. viruses) to issues related to human genetics \citep{wolff2001}.

Despite the above observations, there are many high-frequency terms that are either not very descriptive or their importance is not discernible without substantial knowledge of the context. Moreover, terms that exist well below the 25-word cutoff may be more informative or terms may shift in frequency together in patterns that are difficult to discern ex ante. However, we can potentially overcome these limitations with naive examination of changes in the frequency of language use as I argued earlier in this chapter. Building on the recent work of \citet{klingenstein2014}, I develop and test a measure that tracks the amount of change that occurs from one frequency distribution to the next. The details of this measure are presented in the following sections.

\section{Kullback-Leibler Divergence\label{kld}}

In computational linguistics, a process called topic modeling is gaining in popularity as way to extract meaning from large, unstructured textual databases \citep{blei2012}. In its most basic form, the process involves the naive classification of lexical `features' into buckets based on criteria such as cooccurrence within the text. For instance, the terms `player' and `court' might co-occur in many articles about basketball and thus contribute highly to that topic \citep{blei2009}.\footnote{Note that many topic models can be viewed as a form of principal component analysis given a matrix of articles by terms \citep{blei2012}} Such techniques have been successfully implemented across the social sciences to understand research agendas as varied as the classification of scientific knowledge \citep{blei2007} and brand positions in a competitive market \citep{tirunillai2014}.

Nonetheless, the process under investigation in the current work is dynamic, and the most common methods for naive topic modeling produce classifications that are static across time--the order of documents is irrelevant to the production of the classificatory scheme. Even the dynamic topic model recently proposed by \citet{blei2006} imposes some restrictive assumptions. For instance, the set of features upon which the dynamic model is estimated, must be defined prior to execution of the algorithm \citep{blei2012}. A typical procedure is to choose the total set of features as the union of the top $X$ features by frequency from each time period. However, this allows the most prominent features that exist far into the future (i.e. vocabulary) to influence topics modeled in the distant past. Moreover, the insights one gains from such models are based on how the various features change in their influence of a persistent topic--a worthwhile endeavor, but limiting in the current context where topics (categories) are assumed to emerge, merge and disappear over time \citep{goldberg2012}.

An alternative approach--and the one pursued here--imposes a somewhat less burdensome restriction. Rather than define the set of features using the entire (time-invariant) corpus, I draw from a moving window around the period under consideration. Changes are captured as the difference between features at time $t$ and the features defined by their average over the previous $k$ periods. Thus, I can limit features to the intersection of the top $X$ by frequency in periods $t-k$ through period $t$. Moreover, the $k$-period moving average incorporates innovations gradually so that new shocks are defined against the relevant past--the period of time most likely to exist in an actor's recent memory--rather than the field's entire history. 

The features I consider for my model are word-stems. They are mapped to a probability distribution based on the frequency with which they occur in a given time period. This is sometimes referred to as a `bag-of-words' model because it employs word-stems and their frequencies without consideration of their contextual ordering in sentences, paragraphs, etc \citep{jurafsky2000}. I compute the uniqueness of the distribution at period $t$ by means of the Kullback-Leibler divergence (KLD) from the average of the prior $k$ distributions \citep[see e.g.][]{klingenstein2014}. The KLD is denoted $D_{KL}(P\|Q)$ where $P$ is assumed to represent the ``true" distribution, which in this case, is the distribution constructed from the prior $k$ periods. Q is the distribution at time $t$. For discrete distributions (as I have here) the measure is defined as,

\begin{equation}
	D_{KL}(P_k\|Q_t) = \sum_i{P_k(i)ln\frac{P_k(i)}{Q_t(i)}},
\end{equation}

\noindent which describes the logarithmic difference between the probabilities $P$ and $Q$. The consistency of this measure over time relies on the fact that the shape of the frequency distribution doesn't change appreciably with changes in the text--a fact grounded in Zipf's law \citep{zipf1932, adamic2002}. Thus changes are largely based on a reordering of features within the distribution rather than a reconfiguration of it's shape. Figures \ref{zipf1} and \ref{zipf2} illustrate this fact graphically by showing the the same power law relationship between words and their frequencies despite different orderings and higher frequencies in general for the later time period.

\begin{figure}
\begin{center}
\includegraphics[scale=.5]{../figures/zipf1.png}
\caption[Frequency Distribution 1991]{Frequency Distribution for year 1991 top 50 word stems. \label{zipf1}}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=.5]{../figures/zipf2.png}
\caption[Frequency Distribution 2001]{Frequency Distribution for year 2001 top 50 word stems. \label{zipf2}}
\end{center}
\end{figure}

The KLD quantifies the amount of information lost when $Q$ is used to predict $P$. As such, it captures the new information represented by the current set of word-stem frequencies. When the value is high, it describes a departure from the prior $k$ distributions. However, when the value is low, it represents consistency with prior word-stem frequencies. Because the KLD always takes on values between 0 and 1, I can define a measure of language consistency over time as,

\begin{equation}
	LCON_t=1-D_{KL}(P_k\|Q_t).
\end{equation}


\section{Descriptive Statistics}

For the descriptive statistics and analysis that follow, my measure of language consistency is based on probability distributions derived from the intersection of the top 1,000 word-stems in the current month ($Q_t$) and the prior three ($P_k$). Several other configurations were tested as as part of my robustness checks and the results are qualitatively the same (see Appendix A). Table \ref{freq} shows the average monthly word stems, vocabulary (i.e. unique word-stems) and consistency over the thirteen year period under investigation.

Several trends are immediately apparent. First, the amount of writing grew substantially throughout the 1990's before leveling off around year 2000. Second, the vocabulary used to describe the industry more than doubled over the same period with the sharpest increase occurring between 1992 and 1993. Third, the consistency in the way the language was used rose gradually over the same period. If, as I argued previously, the frequency of language use is indicative of the legitimation process \citep[e.g.][]{petkova2014, rosa1999}, then this rise in consistency suggests that the industry as a whole has undergone a gradual march towards the establishment of greater field-wide knowledge and shared understanding \citep{cattani2008}. That is, the consistency observed in language may proxy for shared understanding--or consensus--at the field level.

\begin{table}
\begin{center}
\caption[Language Frequencies by Year]{Language Frequencies by Year \label{freq}}
\vspace{0.3in}
\begin{tabular}{cccc}
\hline 
\hline
YEAR & WORD-STEMS & VOCABULARY & LCON \\
\hline
1991 & 57,956 & 4816 & .8783283 \\
1992	 & 147,557 & 7,478 & .9098001 \\
1993 &  207,807 & 10,019 & .9397479 \\
1994 &  234,350 & 10,743 & .9359598 \\
1995 &  215,690 & 9,680 & .9437492 \\
1996 &  214,447 & 9,854 & .9425526 \\
1997 &  238,250 & 10,594 & .9438075 \\
1998 &  243,287 & 11,240 & .9512883 \\
1999 &  267,435 & 12,593 & .9478587 \\
2000 &  328,733 & 13,484 & .9557859 \\
2001 &  329,704 & 14,673 & .9540956 \\
2002 &  342,010 & 13,854 & .9625357 \\
2003 &  374,304 & 13,286 & .9642392 \\
\hline
\end{tabular}
\end{center}
\end{table}

Despite the gradual increase in language consistency year over year, there is significant variation at the monthly level. Figure \ref{lcon1} shows the language consistency measure plotted on a monthly basis from 1991 through 2003. Notably, there is significant volatility in the first couple of years, which may be due to institutional factors unrelated to the legitimation process. For instance, new reporters must be recruited and learn how to write about the industry. As mentioned above, the vocabulary more than doubled in these first few years. The results and further discussion are presented after removing these first two years of data. I also present a statistical justification for their removal in Appendix \ref{apndxA}.

%\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[scale=.4]{../figures/lcon1.png}
\caption[Monthly Language Consistency (1991-2004)]{Monthly Language Consistency (1991-2004)\label{lcon1}}
\end{center}
\end{figure}
%\end{landscape}

Figure \ref{lcon2} shows the language consistency measure for the years 1993 onwards in both levels (top) and first differences (bottom). However, there are several additional features worth noting. The upper graph shows several downward spikes, which represent strong deviations from the prior three months of language use. In several cases, this is followed by a rise back to relative consistency suggesting that the change in language was incorporated and adopted. In other words, some industry shift took place, which then became a permanent feature of the lexicon.

One notable exception are the years 1999 through 2001 during which the consistency of the language fluctuates several times. This is most apparent when looking at the (lower) graph in first differences around this time. Clearly the the volatility of this measure is pronounced. This coincides with the general turmoil surrounding the Dotcom bubble and the subsequent spillover into other industries. For instance, many biotech firms went out of business during this period for lack of financing while others sought the safety of alliances with large pharmaceutical companies \citep{wolff2001}.

%\begin{landscape}
\begin{figure}
%\begin{figure}[p!]
\begin{center}
\includegraphics[scale=.4]{../figures/lcon2.png}
\caption[Monthly Language Consistency, Levels and Differences (1993-2004)]{Monthly Language Consistency in Levels and First Differences (1993-2004)\label{lcon2}}
\end{center}
\end{figure}
%\end{landscape}

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[scale=.60]{../figures/lcon3-edit.png}
\caption[Highlighted Deviations in Monthly Language Consistency]{Highlighted Deviations in Monthly Language Consistency\label{lcon3}}
\end{center}
\end{figure}
\end{landscape}

Figure \ref{lcon3} highlights the most significant drops in consistency over the 11 year period (1993-2003) after which there appears a period of relative consistency in the language. The three highlighted deviations occurred in March 1994, January 1997 and July 2001. Listed next to each deviation are the 25 word-stems, which contributed most significantly to the change in probability. The word-stems with positive change (increase in probability mass) are listed on top in green and those with negative changes (decreases in mass) are listed on the bottom in red. 

While it is difficult to draw strong conclusions, a few patterns are apparent. The deviation in 1994 was led by more  sharp declines than gains, with declines outpacing gains nearly 2.6 to 1. The declines suggest a waning attention to the early stages of biotech ventures with the words \emph{stock}, \emph{share}, \emph{chief}, \emph{executive}, \emph{announce} and \emph{agreement} all declining significantly. In contrast, the deviation in January of 1997 posts nearly even gains and declines with the words \emph{trial}, \emph{phase}, \emph{clinic}, and \emph{drug} heralding a possible shift towards the logic of commercialization as promising therapies made their way through the difficult FDA approval process. This shift is further supported by declines in the words \emph{study} and \emph{patent}. The decline of the word \emph{american} is also a signal of the globalization of the industry as previously mentioned.

The deviation in July of 2001 is again primarily driven by sharp declines in probability mass. While difficult to interpret from a reading of the word-stems, a review of the articles appearing around this time suggest a shift away from the science of biotech and towards the regulatory climate within which the industry was operating. This was shortly after George W. Bush started his first term as President of the United States and there was a concern amongst scientists that funding for stem cell research could be cut. This is supported by advances in the words \emph{stem} and \emph{grant}. There was also some discussion as to who would be approved to take over as head of the FDA. Although the word \emph{approve} didn't make the top 25 in absolute value (and thus is not shown in Figure \ref{lcon3}), it is in the top 20 words that made probability gains.


\section{Consensus and the Stock Market}

In Chapter \ref{law} I talked about the relationship between shared understanding (representational homogeny) and exchange.  I proposed that increases in shared understanding would drive exchange in markets characterized by a high degree of knowledge heterogeneity. I also proposed that new information (i.e. decreases in shared understanding) could drive exchange in markets that had reached a state of relative equilibrium. In this section I test these propositions by looking at how my measure of language consistency is related to the trading volume of common shares issued by publicly-traded biotech firms.

The set of firms used is taken from the dataset collected by Powell and his colleagues \citep[see][]{powell2005, powell1996}, extended through 2003. Stock market data is collected from the Wharton Research Data Service (CRSP/Compustat merged). Additional information on this dataset is presented in Chapter \ref{signal}; however a few points are worth mentioning here. The database focuses on dedicated human biotech firms, omitting companies involved in veterinary and agricultural biotech (which draw on different scientific capabilities and operate in a much different regulatory climate). The sub-sample used here covers only publicly traded firms, of which 153 were in existence in 1993 and 252 in 2003. In this time-frame, some firms were created and entered the database, and others exited, due to failure, departure from the industry, or acquisition. 

To reduce volatility in my aggregate measure of trading volume and for econometric reasons clarified in Chapter \ref{signal}, I dropped firms with less than three years of trading data from the sample.\footnote{Leaving all firms in for this analysis made no qualitative difference in the results.} The final sample numbers between 111 and 192 firms in any given year. Trading volume is operationalized as the log of the sum of the trading volume for all active firms in a given month (denoted $LVOL_t$). As such it captures general interest and activity in the biotech sector in proportion to firm size. This is in contrast to a measure like turnover, which captures valuation-independent trading activity. The two measures are related; however, in this context one can think of my volume measure like that which is associated with a sector mutual fund or a portfolio, which are often size-weighted to begin with \citep{kim1991}. Further analysis at the firm level can be found in Chapter \ref{signal}.

Figure \ref{vol1} shows standardized measures of trading volume and language consistency on the same axis. Despite significant volatility, both time series seem to adhere to the same long-term average. This is a striking feature of the data and suggests the two series may be cointegrated--that is, each may have a causal effect on the other as both conform to a longer-term trend. If trading volume deviates from the trend, it is ``pulled" back by language consistency just as consistency is ``pulled" back itself by higher trading volume. This adherence to the longer-term average and the interplay of the two variables can be more easily seen in Figure \ref{vol2}, which shows the 3-month moving average of the same variables. 


\begin{figure}
\begin{center}
\includegraphics[scale=.4]{../figures/vol1.png}
\caption[Std. Trading Volume and Language Consistency]{Standardized Trading Volume and Language Consistency\label{vol1}}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=.4]{../figures/vol2.png}
\caption[Std. Trading Volume and Language Consistency (3-mos. MA)]{Standardized Trading Volume and Language Consistency (3-month MA)\label{vol2}}
\end{center}
\end{figure}

When analyzing two time-series variables that appear nonstationary (i.e. trending), it is common practice to look at their relationship in first differences, which are assumed to be covariance stationary \citep{phillips1988}. However, if the variables cointegrate (as I suspect given the plots), then even a simple regression in differences can be misspecified yielding spurious results \citep{granger1974}. In these cases it is important to test wether some linear combination of the series is stationary even though each series is not stationary by itself \citep{engle1987, johansen1988}. Following the work of \citet{trusov2009}, \citet{xiong2013} and others I first use Aikake Information Criterion (AIC) to identify the lag order of the combined series, which is three in this case. I then test for cointegration using the methods described in \citet{johansen1988, johansen1995} with three lags. The trace statistic of $0.3018$ is well below the $5\%$ critical value of $3.76$ used for a regression with 13 parameters, which allows me to reject the null hypothesis of no cointegration.

In the presence of cointegration, the familiar vector autoregression concept \citep[see e.g.][]{trusov2009} can be extended to a vector error-correction model, or VECM. The model is fit to the first differences of the nonstationary variables, but a lagged error-correction term is added to account for the long-term relationship. In the case of two variables, the added term is the lagged residual from the cointegrating regression of one of the series on the other in levels. Thus, it expresses the prior disequilibrium from the long-run relationship, in which that residual would be zero. Allowing for a constant and a linear trend, the error correction model for a two variable case can be expressed as follows:

\begin{equation}
	\Delta LVOL_t = \psi_1\mu_{1t-1}+\sum_{i=1}^mA_i\Delta LVOL_{t-i} + \sum_{j=1}^mB_j\Delta LCON_{t-j}+v_1+\delta_1 t+\epsilon_{1t}
\end{equation}
\begin{equation}
	\Delta LCON_t = \psi_2\mu_{2t-1}+\sum_{i=1}^mC_i\Delta LVOL_{t-i} + \sum_{j=1}^mD_j\Delta LCON_{t-j}+v_2+\delta_2 t+\epsilon_{2t},
\end{equation}

\noindent where $LVOL$ and $LCON$ are the trading volume and language consistency measures respectively. The error correction terms $\mu_{t-1}$, are the residuals from the cointegrating regression (as mentioned above). Note that these terms represent the extent of disequilibrium in levels from the prior period. Thus, the VECM specification states that changes in $LVOL$ and $LCON$ not only depend on each other and their own past differences, but also on the extent to which the two variables have deviated from their long-term average. Note also that the constant term $v$ implies a linear time trend in the levels, and the time trend $\delta t$ implies a quadratic time trend in levels. This allows for added flexibility in the shape of the long-term trend. $m$ is the number of lags as determined by the AIC (as above).

Estimation of the two variable VECM was accomplished using the maximum likelihood methods developed by \citet{johansen1995} and implemented in Stata 13 using the \emph{vec} command. Coefficients and their z-values from this estimation are shown in Table \ref{vec}. Both the $LVOL$ and the $LCON$ equations are well specified with $P>\chi^2$ well below the $5\%$ cutoff and R-squared values of $0.13$ and $0.34$ respectively.\footnote{I also verified the absence of autocorrelation in the residuals using the typical methods \citep[see e.g.][]{trusov2009}} The cointegrating equation is also highly significant $(\chi^2 = 57, df=1)$ as are each of the error correction coefficients shown as $L.\_ce1$ in Table \ref{vec}. The fact that these coefficients have opposing signs is a requirement for a well-specified VECM model: that is, if $(LVOL-LCON)$ is above the long term average, either $LVOL$ must fall or $LCON$ must rise. 

\begin{table}
\begin{center}
\caption[VECM Results]{Results of Vector Error Correction Model \label{vec}}
\vspace{0.3in}
{
\def\sym#1{\ifmmode^{#1}\else\(^{#1}\)\fi}
\begin{tabular}{l*{1}{cc}}
\hline\hline
&       Coef.           & z         \\
\hline
D\_LVOL    &                  &       \\
L.\_ce1    &  -0.0840\sym{***}&  (-3.44)\\
& & \\
LD.LVOL   &  -0.0670         &  (-0.80)\\
L2D.LVOL  &   -0.135         &  (-1.62)\\
& & \\
LD.LCON   &   -8.175\sym{**} &  (-3.17)\\
L2D.LCON  &   -3.628         &  (-1.59)\\
& & \\
\_cons    &0.0000534         &   (0.00)\\
\hline
D\_LCON    &                  &         \\
L.\_ce1    &  0.00378\sym{***}&   (3.92)\\
& & \\
LD.LVOL   & -0.00349         &  (-1.06)\\
L2D.LVOL  & -0.00840\sym{*}  &  (-2.55)\\
& & \\
LD.LCON   &   -0.109         &  (-1.07)\\
L2D.LCON  &   -0.231\sym{*}  &  (-2.57)\\
& & \\
\_cons    &  0.00119         &   (1.84)\\
\hline
\(N\)     &      129         &         \\
\hline\hline
\multicolumn{3}{l}{\footnotesize \textit{z} statistics in parentheses}\\
\multicolumn{3}{l}{\footnotesize \sym{*} \(p<0.05\), \sym{**} \(p<0.01\), \sym{***} \(p<0.001\)}\\
\end{tabular}
}
\end{center}
\end{table}

There is a strong connection between cointegration and causality in that at least one granger-causal relationship must exist in a cointegrated system \citep{engle1987}. In this case the long-term causality appears to flow both directions as each variable ``pulls" the other back towards equilibrium. This can be seen visually by looking at the orthogonalized impulse response functions (OIRF), which show one variable's response to a one-unit increase in the other.\footnote{The interpretation of OIRFs rely on the stability of the model. The companion matrix of a VECM with $K$ endogenous variables and r cointegrating equations has $K - r$ unit eigenvalues. If the process is stable, the moduli of the remaining r eigenvalues are strictly less than one, which I verified using the \emph{vecstable} command in Stata 13} Figure \ref{irf1} shows the effect of $LCON$ on $LVOL$ and Figure \ref{irf2} shows the effect of $LVOL$ on $LCON$. 

\begin{figure}
\begin{center}
\includegraphics[scale=.6]{../figures/irfc1.png}
\caption[Orthoganalized Impulse Response (LCON on LVOL)]{Orthoganalized Impulse Response (LCON on LVOL)\label{irf1}}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=.6]{../figures/irfc2.png}
\caption[Orthoganalized Impulse Response (LVOL on LCON)]{Orthoganalized Impulse Response (LVOL on LCON)\label{irf2}}
\end{center}
\end{figure}

Unlike a vector autoregresive model, the variables in a cointegrating VECM are not mean reverting, so the effects of some shocks will not die out over time. This can be clearly seen in the OIRFS from Figures \ref{irf1} and \ref{irf2}. An increase in language consistency has a long-term permanent effect on trading volume, and similarly, an increase in trading volume appears to have a permanent effect on language consistency. Both variables are ``pulling" each other higher in accordance with their long-term average.

The remaining coefficients in Table \ref{vec} also provide evidence of short-term causal effects. The coefficients prefixed with $LD$ and $L2D$ are the effects of first and second lagged variable differences on themselves and on each other. $LCON$ has a negative short-term effect on $LVOL$ in the first lag and $LVOL$ has a negative short-term effect on $LCON$ in its second lag. The short-term destabilizing effect of an increase in trading volume on language consistency is also apparent in the levels equation (though insignificant at 95\% confidence) as can be seen in Figure \ref{irf2}; however, the short-term negative effect of $LCON$ on $LVOL$ in differences is particularly intriguing. Recall from my discussion in section \ref{kld} that the KLD captures the new information represented by the current set of word frequencies (as compared to the last three months). Language consistency was defined as $1-KLD$, or essentially, the \emph{lack} of information in the current set of word-stems. The negative short-term effect of $LCON$ on $LVOL$ in the VECM is thus the equivalent of saying that new information (a drop in $LCON$) granger causes a transitory spike in trading volume.

These two distinct effects--the short-term and the long-term--describe a nuanced system of relationships between language consistency and trading volume. While both variables tend to elevate each other in an increasing long-term trend, \emph{drops} in consistency (i.e. new information) can also increase trading volume, if only briefly. In a general sense, this is consistent with the propositions developed in Chapter \ref{law}. As knowledge in the  biotechnology industry becomes ``common" knowledge over time, the liquidity of (and possibly the demand for) a firm's shares can increase. This occurs because more investors understand the nature of the investment. However, in the shorter-term, new information can jolt trading volume from it's long term equilibrium in a transitory spike of increased activity \citep{fama1969}.

Despite these intriguing results, many questions remain. For instance, does the effect that language consistency has on exchange affect firm performance? In Chapter \ref{signal}, I address this question by building and testing theory on the relationship between language consistency, and the advantages derived from a firm's centrality in a network of long-term exchange relationships. Language consistency is shown to moderate the effect of a firm's network position on a variety of measures of firm performance.


