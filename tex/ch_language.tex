\chapter{Legitimacy and Language\label{lang}}

\begin{small}
\begin{quote}
``Grammars in both cooking and engineering exist not just as rules but as a set of unspoken practices taken for granted" (Arthur 2009 p. 77)
\end{quote}
\end{small}
Chapter \ref{law} introduced a concept called `market entropy' as a way to describe the degree to which actors are similar or different in their representations of the market. This is a broad concept which incorporates insights from several established literatures published under the neo-institutionalist moniker (e.g. Denzau and North 1994; DiMaggio and Powell 1983; Loasby 1999). In this chapter, I narrow the scope of investigation slightly to focus on one of these literatures; namely, research on legitimacy and the legitimation process. 

Legitimacy can be defined as ``consensus among agents (audiences) that the features and activities of [market actors] are appropriate and desirable within a widespread, taken-for-granted system of norms or social codes" (Cattani et al. 2008, p. 147). Legitimacy is distinct from the more general `market entropy' construct in that it imposes a moral dimension on shared understanding--that is, behavior is not just `taken-for-granted' but also `appropriate.' Yet, the substantial body of research on the legitimation \emph{process} can help us understand how shared understandings actually come about in practice (Colyvas and Powell 2008; Goldberg 2013).

Typically, legitimation occurs over a long period of time and involves many actors (Rosa et al. 1999). The process eventually leads to the establishment of field-wide knowledge and shared understanding of the boundaries and definition of relevant technological dimensions (Bijker, Hughes, and Pinch, 1987; Santos and Eisenhardt, 2009), the meaning of new product categories (Kennedy, Lo, and Lounsbury, 2010; Rosa et al., 1999), and the validity of new practices (Lounsbury and Crumley, 2007; Tripsas, 2009). 

Nonetheless, as a legitimation process unfolds, there are often periods of high disagreement (e.g. Colyvas and Powell 2008). During these periods, actor's representations of the market are either in conflict or not sufficiently diffuse. Through a process of negotiation and diffusion, dominant understandings (designs, practices, etc.) emerge as the `taken-for-granted' within a field (Cattani et al. 2008). This is the framework within which subsequent developments are then judged.

This formulation of shared understanding--derived from consideration of the legitimation process--comports nicely with my focus on field-level transitions in technology, taste and attention. However, a brief review of the empirical literature on legitimacy raises an additional concern: the behavior deemed legitimate is typically defined ex post--that is, operationalizations are based on the present incarnation of a legitimized category and then traced to some earlier period. For example, Petkova et al., (2014) examine the legitimacy of investing in the clean energy sector by counting historical references to the terms ``clean energy," ``green energy" and ``alternative energy" in media articles. In practice, categories emerge organically (Goldberg 2013) and may cycle through several incarnations of appropriate behavior before they settle on the perspicuously demarcated boundaries that appear to the present observer. More importantly, this formulation excludes the struggle between competing paradigms that characterizes the typical legitimation process (Colyvas and Powell 2008).

How then, do we attempt to understand transitions in technology, taste and attention in an unbiased manner? Despite it's shortcomings, the literature on legitimacy provides several useful insights. Foremost, the frequency of  behavior is a meaningful yardstick of general acceptance (Hannan and Carroll, 1992). Second, the frequencies of language used in the media are representative of an unfolding legitimation process (Petkova et al., 2014; Rosa et al., 1999). Using these insights, I propose a general notion of shared understanding based on changes in the frequency of language use. This can be accomplished without imposing ex post classifications by tracking language frequencies as they naturally occur--that is, the frequency distribution of descriptive words at various points in time. Changes in a frequency distribution from one period to the next is thus assumed to represent shifts in the importance of behavior described by the language. In contrast, similar distributions from one period to the next may represent an emerging consensus in the field. 

\section{Language Models}

A variety of recent studies have started to look at the generic characteristics of written language as a way to understand the structure of markets and the evolution of human behavior (Klingenstein, Hitchcock, and DeDeo 2014; Goldberg 2013; Tirunillai and Tellis 2014). The viability of this type of research is driven by parallel developments in the availability of digital archives and methods appropriate for the analysis of ``big data." While the standards and practices used for large-scale textual analysis are still very much under development, I have largely followed the work of prior scholars in preparing my data and constructing my measures. The following paragraphs describe this process in detail.

Textual data is sourced from articles written for the Bioworld trade journal between January 1991 and December 2004. During this period, Bioworld constituted a primary source for industry-wide dissemination of information about the activities of firms and other stakeholders in the biotechnology industry. They published daily articles (excluding weekends) on topics as wide-ranging as personnel changes, fundraising activity, inter-firm contracting, the FDA approval process and the latest scientific trends. Below are two different example articles--the first from 1994 and the second from 2004--that illustrate the level of detail typically present in the text.

\begin{singlespace}
\begin{small}
\begin{quotation}
\noindent \textbf{Teaching Cells To Make Their Own Anti-Aids Interferon} \\

It's one thing to administer recombinant interferon as antiviral therapy. Educating cells to make their own do-it-yourself interferon is something else. A team of French scientists reports doing just that to hamstring the AIDS virus. Their paper in the current Proceedings of the National Academy of Sciences (PNAS) describes: ``Blocking of retroviral infection at a step prior to reverse transcription in cells transformed to constitutively express interferon b." The group's leader, Edward DeMaeyer of France's National Scientific Research Institute (INRA) stated, ``We are developing methods for somatic-cell gene therapy directed against infection with human immunodeficiency virus ...through the constitutive production of autocrine interferon (IFN)." His strategy aims to block HIV at its earliest stages of cell entry and replication, rather than the later stages at which interferon is commonly thought to act.

For starters, the team transfected various mouse and human cell lines with IFN genes. These transformed cells synthesized interferon at a constant low rate; their replication and survival were the same as in control cells. When challenged with various retroviruses, they markedly reduced the number of virally infected cells. Then the team assailed CEM cells with HIV viral particles. (CEM is a model human T-cell line derived from patients with leukemia.) ``Two days later," as DeMaeyer reported, ``the presence of HIV-1 proviral DNA was significantly lower in the human IFN-b transformed cells than in the control cells, indicating an early block of the infectious cycle."

Six hours after the onset of infection, 40 to 80 percent of the viral particles could still be found in the transformed cells' culture medium, whereas 90 percent of the control cells' virions had moved on to infect other cells. But the transformed CEM cells died within a week or so. ``These findings," DeMaeyer concluded, are encouraging for the use of the ...IFN-b vector to explore the possibility of developing an anti-HIV-b-directed somatic cell gene therapy." Richard Mulligan at MIT's Whitehead Institute of Biomedical Research supplied the vector plasmids to the French investigators.

SInce submitting his findings to PNAS last September, DeMaeyer has determined that virus-resistant, inferferon-synthesizing CEM cells ``survive for as long as 80 days. Maybe they lived even longer," he told BioWorld in a telephone interview, ``but that's when we stopped the experiment." Moreover, moving from off-the-shelf CEM cell lines, he and his group have shown ``that the same effect is obtained in fresh peripheral blood lymphocytes taken from people. Of course," he added, "this is more
significant in terms of somatic gene therapy. That's the cell that one wants eventually to protect in the body."

The next step, he said, is to proceed from in vitro to in vivo testing. ``We want to collaborate with a group that has the rhesus monkey model going, infectable with the simian immune deficiency virus, SIV." He is already in discussion with one such research center, but is still open to other offers.

DeMaeyer cited three goals of such testing: First, to show that it works in vivo; second, to determine that the transformed cell's
immunological function is not impaired; third, to solve the problem of ``getting enough of those interferon-transformed, virus-resistant cells into the body of a seropositive person infected with AIDS."\\

\noindent \textbf{Threshold Pharmaceuticals Inc. began patient enrollment in a Phase III trial of glufosfamide for pancreatic cancer.} \\

The pivotal study is designed to test the company's lead product in patients with metastatic pancreatic cancer refractory to first-line treatment. Its primary endpoint will compare the median survival of patients treated with glufosfamide and best supportive care to those solely receiving best supportive care - all medical or surgical interventions that a pancreatic cancer patient should receive to palliate the cancer but excluding treatment with systemic therapies intended to kill the cancer cells.

At present, there are no approved therapies for those patients, who generally have an expected survival of about three months. ``In fact, one would really say there are no proven options for systemic treatment of their cancer," Threshold President George Tidmarsh told \emph{BioWorld Today}. Of the 31,860 patients expected to be diagnosed with pancreatic cancer in the U.S. this year, about 31,270 will die from the disease, according to American Cancer Society statistics.

The South San Francisco-based company agreed with the FDA on a special protocol assessment, which indicates that if the trial successfully meets its primary endpoint, the data will provide support for an efficacy claim in an eventual marketing application. Tidmarsh said the FDA requested only a few minor wording changes to Threshold's study design, case-reporting system and statistical analyses plans as both parties came to an agreement in the process.

Enrollment is under way in the U.S., while more sites will be added in Eastern Europe and South America. Eventually the trial will involve about 306 patients; recruitment is expected to last between 12 months and 15 months. Eligible patients will be randomized to receive glufosfamide every three weeks in addition to best supportive care, or best supportive care alone.

``There is a planned analysis on an interim data set that we project to have in about 15 months," Tidmarsh said, noting that the timeline would depend on enrollment. ``Our anticipation is that the complete data, with all finalized study reports, would be available in approximately 18 to 24 months."

Secondary endpoints in the trial include disease-free survival and time to disease progression, as well as response and safety evaluations. A next-generation chemotherapeutic agent from the alkylator class, it is designed to enter cells through up-regulated glucose transport proteins.

``The toxin is attached to the glucose molecule, so we believe that it allows the drug to target specifically to the cancer through a mechanism we call metabolic targeting," Tidmarsh said. ``And metabolic targeting is based on the dramatic increased consumption of glucose that tumor cells have relative to normal cells."

He added that glufosfamide produces fewer side effects than prior-generation alkylators, which generated toxins that led to hemorrhagic cystitis. Prior studies have provided a glimpse at the drug's potential efficacy in fighting pancreatic cancer.

Data from a Phase II trial, reported at the America Association for Cancer Research meeting in 2002 and published in the November 2003 issue of the \emph{European Journal of Cancer}, revealed objective tumor shrinkage. An updated analysis of the survival shows an estimated 9 percent two-year survival, which the company said compares to 1 percent or less in historical studies with other first-line therapies. The fully enrolled study included 35 chemotherapyna ve patients with locally advanced or metastatic pancreatic cancer.  

Findings from an initial Phase I trial, published in the October 2000 edition of the \emph{Journal of Clinical Oncology}, showed that its only pancreatic cancer participant achieved a complete remission of disease and remained in remission more than five years later after receiving glufosfamide treatment alone.

Threshold owns all rights to the compound, and Tidmarsh said the company would continue to steer development on its own. It also has demonstrated glufosfamide's activity in Phase II studies in breast and colon cancers, findings which were reported at this year's American Society of Clinical Oncology meeting, and plans to explore its use in lymphoma.

Outside of glufosfamide's development, Threshold is developing two other clinical-stage products, both of which also are based on the company's metabolic targeting approach. A compound labeled TH-070 is in Phase II for benign prostatic hyperplasia, while its 2-deoxyglucose product is in Phase I for solid tumors.

Earlier this year, Threshold filed to go public, though its initial public offering has yet to price. (See \emph{BioWorld Today}, April 15, 2004.) 
\end{quotation}
\end{small}
\end{singlespace}

With even a quick glance at the prior two articles, one will notice several prominent features of the text. First, there are a large number of industry-specific words dealing with the science of biotechnology--words like `glufosfamide' and `virion.' These words are very specific and exist at a very low frequency in the text. Because of this, they provide little insight into the broad subject matter of the article. In contrast, words like `the,' `to,' `is,' `of,' and `was' show up everywhere. However, these words constitute the ``plumbing" of the english language and are also uninformative. Between these two extremes, are words like `HIV,' `cancer' and `cell.' These words are descriptive, but not overly specific. Moreover, they appear with sufficient periodicity to suggest they are an integral part of subject matter under consideration. 

What this very rudimentary exercise has demonstrated, is that we can learn something of importance simply by looking at the frequency at which a word appears in an article. In particular, if we can capture changes in the most informative set of descriptive words over time, we might be able to infer something about how the industry is changing. For instance, perhaps the importance of \emph{HIV} relative to \emph{cancer} shifted in the period between 1994 and 2004.

\section{Frequency Distributions}

Between 1991 (when the journal first started) and the end of 2004, Bioworld published 22,461 articles. Following the work of Klingenstein et al., (2014), Tirunillai and Tellis (2014) and others, I performed the following steps on each article in order to prepare the data for further analysis.
\begin{enumerate}
  \item Break apart each article into a list of lexical elements at the word level in a process typically referred to as tokenization (Jurafsky and Martin 2000). \footnote{Note for instance that compound words are split into their constituent parts so that a word like `don't' becomes three elements: `don,' `',' and `t.'}
  \item Remove all punctuation and numbers from the list.
  \item Change all elements to lower case.
  \item Remove all English `stop words' (i.e. the plumbing)--see table \ref{stop}--and words of length $< 3$.
  \item Stem the remaining elements using the Porter (1980) stemmer so that words like `work' and `working' are treated as the same lexical element.
\end{enumerate}

\begin{table}
\begin{center}
\caption[English Stopwords]{English Stopwords (i.e. the ``plumbing"). \label{stop}}
\vspace{0.3in}
\begin{tabular}{ccccc}
\hline 
\hline
i & me & my & myself & we \\
our & ours & ourselves & you & your \\
yours & yourself & yourselves & he & him \\
his & himself & she & her & hers\\
herself & it & its & itself & they\\
them & their & theirs & themselves & what \\
which & who & whom & this & that \\
these & those & am & is & are\\
was & were & be & been & being \\
have & has & had & having & do\\
does & did & doing & a & an \\
the & and & but & if & or \\
because & as & until & while & of \\
at & by & for & with & about \\
against & between & into & through & during \\
before & after & above & below & to \\
from & up & down & in & out \\
on & off & over & under & again \\
further & then & once & here & there \\
when & where & why & how & all \\
any & both & each & few & more \\
most & other & some & such & no \\
nor & not & only & own & same \\
so & than & too & very & s \\
t & can & will & just & don \\
should & now & & & \\
\hline
\end{tabular}
\end{center}
\end{table}

Following the preprocessing steps described above, the lists of lexical elements representing each article were pooled by month into 168 larger lists--one for each month between January 1991 and December 2004. \footnote{Various aggregation sizes were tested and the results (shown later) remain qualitatively the same.} From these lists, I then constructed 168 frequency distributions, with the lexical element in rank order on the x-axis and its count divided by the total number of lexical elements constituting the y-axis--that is, that element's probability given the total list of elements in that month. Tables \ref{topwords1} and \ref{topwords2} list the top 25 word stems in rank order for each of the 14 years under consideration. 

\begin{table}
\footnotesize
\begin{center}
\caption[Top Words (1991-1997)]{Top Words (1991-1997) \label{topwords1}}
\vspace{0.3in}
\begin{tabular}{ccccccc}
\hline 
\hline
1991 & 1992 & 1993 & 1994 & 1995 & 1996 & 1997 \\
said & said & compani & said & said & said & said \\
compani & compani & said & compani & compani & million & compani \\
million & right & develop & drug & million & gene & cell \\
develop & million & cell & develop & drug & drug & drug \\
share & develop & right & million & cell & compani & gene \\
right & product & health & trial & develop & cell & million \\
health & health & million & health & gene & develop & develop \\
drug & share & product & right & trial & patient & trial \\
american & american & american & cell & product & protein & patient \\
stock & consult & drug & research & patient & year & product \\
product & drug & research & product & right & product & protein \\
consult & cell & trial & gene & research & studi & research \\
reserv & reserv & patient & patient & health & research & cancer \\
market & trial & share & american & studi & right & phase \\
cell & stock & consult & share & year & trial & human \\
nasdaq & nasdaq & reserv & consult & american & diseas & diseas \\
corp & market & gene & year & share & share & year \\
research & research & clinic & reserv & phase & human & studi \\
patent & patient & presid & phase & would & cancer & clinic \\
bioworld & presid & stock & studi & diseas & health & percent \\
trial & corp & also & clinic & consult & american & use \\
percent & technolog & diseas & bioworld & cancer & phase & share \\
year & year & technolog & would & reserv & percent & also \\
presid & clinic & nasdaq & human & percent & consult & pharmaceut \\
close & patent & market & market & human & also & treatment \\
\hline
\end{tabular}
\end{center}
\end{table}
\begin{table}
\footnotesize
\begin{center}
\caption[Top Words (1998-2004)]{Top Words (1998-2004) \label{topwords2}}
\vspace{0.3in}
\begin{tabular}{ccccccc}
\hline 
\hline
1998 & 1999 & 2000 & 2001 & 2002 & 2003 & 2004 \\
said & said & said & said & said & said & said \\
gene & compani & compani & develop & compani & compani & compani \\
cell & gene & million & compani & develop & million & escap \\
compani & cell & gene & million & drug & develop & million \\
million & million & develop & cell & patient & escap & develop \\
drug & drug & cell & drug & trial & drug & patient \\
develop & develop & drug & patient & million & patient & drug \\
patient & patient & product & product & studi & bioworld & trial \\
protein & trial & protein & trial & product & product & phase \\
product & product & trial & gene & phase & trial & product \\
trial & protein & patient & research & cell & studi & bioworld \\
cancer & phase & technolog & phase & research & phase & studi \\
year & cancer & share & studi & cancer & today & today \\
human & research & phase & technolog & technolog & research & year \\
research & year & research & cancer & diseas & cancer & cancer \\
phase & studi & diseas & diseas & also & cell & research \\
diseas & human & cancer & protein & year & percent & also \\
studi & diseas & human & human & percent & year & percent \\
use & percent & studi & year & today & also & share \\
today & use & use & treatment & pharmaceut & diseas & diseas \\
technolog & technolog & year & also & would & share & pharmaceut \\
percent & share & stock & percent & bioworld & technolog & locat \\
bioworld & stock & therapeut & use & gene & would & clinic \\
market & sequenc & pharmaceut & therapeut & clinic & stock & technolog \\
treatment & pharmaceut & clinic & pharmaceut & use & pharmaceut & cell \\
\hline
\end{tabular}
\end{center}
\end{table}

While it is impossible to draw strong conclusions from a brief scan of the word stems in Tables \ref{topwords1} and \ref{topwords2}, a few trends are apparent. For example, the  term `american' is in the top 10 for the years 1991-1993 before falling in rank in the years and 1994-1996 and then off the list completely by 1997. This comports with observations about the globalization of the biotech industry during these years as noted elsewhere in the literature (e.g. Powell et al. 1999). Second, the term `cancer' doesn't enter the list until 1995 after which point it holds a steady position in the top 20 words throughout the rest of the sample. This is consistent with reports that the science of Biotech has gradually shifted from a focus on infectious diseases (e.g. viruses) to issues related to human genetics (Wolfe 2001).

Despite the above observations, there are many high-frequency terms that are either not very descriptive or their importance is not discernible without substantial knowledge of the context. Moreover, terms that exist well below the 25-word cutoff may be more informative or terms may shift in frequency together in patterns that are difficult to discern ex ante. However, we can potentially overcome these limitations with naive examination of changes in the frequency of language use as I argued earlier in this chapter. Building on the recent work of Klingenstein et al. (2014), I develop and test a measure that tracks the amount of change that occurs from one frequency distribution to the next. The details of this measure are presented in the following sections.

\section{Kullback-Leibler Divergence}

In computational linguistics, a process called topic modeling is gaining in popularity as way to extract meaning from large, unstructured textual databases (Blei 2012). In its most basic form, the process involves the naive classification of lexical `features' into buckets based on criteria such as cooccurrence within the text. For instance, the terms `player' and `court' might co-occur in many articles about basketball and thus contribute highly to that topic (Blei and Lafferty 2009).\footnote{Note that many topic models can be viewed as a form of principal component analysis given a matrix of articles by terms Blei (2012)} Such techniques have been successfully implemented across the social sciences to understand research agendas as varied as the classification of scientific knowledge (Blei and Lafferty 2007) and brand positions in a competitive market (Tirunillai and Tellis 2014).

Nonetheless, the process under investigation in the current work is dynamic, and the most common methods for naive topic modeling produce classifications that are static across time--the order of documents is irrelevant to the production of the classificatory scheme. Even the dynamic topic model recently proposed by Blei and Lafferty (2006) imposes some restrictive assumptions. For instance, the set of features upon which the dynamic model is estimated, must be defined prior to execution of the algorithm (Blei 2012). A typical procedure is to choose the total set of features as the union of the top $X$ features by frequency from each time period. However, this allows the most prominent features that exist far into the future (i.e. vocabulary) to influence topics modeled in the distant past. Moreover, the insights one gains from such models is based on how the various features change in their influence of a persistent topic--a worthwhile endeavor, but limiting in the current context where topics (categories) are assumed to emerge, merge and disappear over time (e.g. Colyvas and Powell 2008).

An alternative approach--and the one pursued here--imposes a somewhat less burdensome restriction. Rather than define the set of features using the entire (time invariant) corpus, I draw from a moving window around the period under consideration. Changes are captured as the difference between features at time $t$ and the features defined by their average over the previous $k$ periods. Thus, I can limit features to the intersection of the top $X$ by frequency in periods $t-k$ through period $t$. Moreover, the $k$-period moving average incorporates innovations gradually so that new shocks are defined against the relevant past--the period of time most likely to exist in an actor's recent memory--rather than the field's entire history. 

The features I consider for my model are word-stems. They are mapped to a probability distribution based on the frequency with which they occur in a given time period. This is sometimes referred to as a `bag-of-words' model because it employs word-stems and their frequencies without consideration of their contextual ordering in sentences, paragraphs, etc (Jurafsky and Martin 2000). I compute the uniqueness of the distribution at period $t$ by means of the Kullback-Leibler divergence (KLD) from the average of the prior $k$ distributions (see e.g Klingenstein et al., 2014). The KLD is denoted $D_{KL}(P\|Q)$ where $P$ is assumed to represent the ``true" distribution, which in this case, is the distribution constructed from the prior $k$ periods. Q is the distribution at time $t$. For discrete distributions (as I have here) the measure is defined as,

\begin{equation}
	D_{KL}(P_k\|Q_t) = \sum_i{P_k(i)ln\frac{P_k(i)}{Q_t(i)}},
\end{equation}

\noindent which describes the logarithmic difference between the probabilities $P$ and $Q$. The consistency of this measure over time relies on the fact that the shape of the frequency distribution doesn't change appreciably with changes in the text--a fact grounded in Zipf's law (Zipf 1932; Adamic and Guberman 2002). Thus changes are largely based on a reordering of features within the distribution rather than a reconfiguration of it's shape. Figures \ref{zipf1} and \ref{zipf2} illustrate this fact graphically by showing the the same power law relationship between words and their frequencies despite different orderings and higher frequencies in general for the later time period.

\begin{figure}
\begin{center}
\includegraphics[scale=.4]{../figures/zipf1.png}
\caption[Frequency Distribution 1991]{Frequency Distribution for year 1991 top 50 word stems. \label{zipf1}}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=.4]{../figures/zipf2.png}
\caption[Frequency Distribution 2001]{Frequency Distribution for year 2001 top 50 word stems. \label{zipf2}}
\end{center}
\end{figure}

The KLD quantifies the amount of information lost when $Q$ is used to predict $P$. As such, it captures the new information represented by the current set of word-stem frequencies. When the value is high, it describes a departure from the prior $k$ distributions. However, when the value is low, it represents consistency with prior word-stem frequencies. Because the KLD always takes on values between 0 and 1, I can define a measure of language consistency over time as,

\begin{equation}
	LCON_t=1-D_{KL}(P_k\|Q_t).
\end{equation}


\section{Descriptive Statistics}

For the descriptive statistics and analysis that follow, my measure of language consistency is based on probability distributions derived from the intersection of the top 1,000 word-stems in the current month ($Q_t$) and the prior three ($P_k$). Several other configurations were tested as as part of my robustness checks and the results are qualitatively the same. Table \ref{freq} shows the average monthly word stems, vocabulary (i.e. unique word-stems) and consistency over the fourteen year period under investigation.

Several trends are immediately apparent. First, the amount of writing grew substantially throughout the 1990's before leveling off around year 2000. Second, the vocabulary used to describe the industry more than doubled over the same period with the sharpest increase occurring between 1992 and 1993. Third, the consistency in the way the language was used rose gradually over the same period. If, as I argued previously, the frequency of language use is indicative of the legitimation process (e.g. Pektrova et al. 2014; Rosa et al 1999), then this rise in consistency suggests that the industry as a whole has undergone a gradual march towards the establishment of greater field-wide knowledge and shared understanding (Cattani et al., 2008). That is, the consistency observed in language may proxy for shared understanding--or consensus--at the field level.

\begin{table}
\begin{center}
\caption[Language Frequencies by Year]{Language Frequencies by Year \label{freq}}
\vspace{0.3in}
\begin{tabular}{cccc}
\hline 
\hline
YEAR & WORD-STEMS & VOCABULARY & LCON \\
\hline
1991 & 57,956 & 4816 & .8783283 \\
1992	 & 147,557 & 7,478 & .9098001 \\
1993 &  207,807 & 10,019 & .9397479 \\
1994 &  234,350 & 10,743 & .9359598 \\
1995 &  215,690 & 9,680 & .9437492 \\
1996 &  214,447 & 9,854 & .9425526 \\
1997 &  238,250 & 10,594 & .9438075 \\
1998 &  243,287 & 11,240 & .9512883 \\
1999 &  267,435 & 12,593 & .9478587 \\
2000 &  328,733 & 13,484 & .9557859 \\
2001 &  329,704 & 14,673 & .9540956 \\
2002 &  342,010 & 13,854 & .9625357 \\
2003 &  374,304 & 13,286 & .9642392 \\
2004 &  350,575 & 12,525 & .9612578 \\
\hline
\end{tabular}
\end{center}
\end{table}



Despite the gradual increase in language consistency year over year, there is significant variation at the monthly level. Figure \ref{lcon1} shows the language consistency measure plotted on a monthly basis from 1991 through 2004. Notably, there is significant volatility in the first couple of years, which may be due to institutional factors unrelated to the legitimation process. For instance, new reporters must be recruited and learn how to write about the industry. As mentioned above, the vocabulary more than doubled in these first few years. 

%\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[scale=.4]{../figures/lcon1.png}
\caption[Monthly Language Consistency (1991-2004)]{Monthly Language Consistency (1991-2004)\label{lcon1}}
\end{center}
\end{figure}
%\end{landscape}

Figure \ref{lcon2} removes the first two years of data and shows the language consistency measure for the years 1993 onwards in both levels (top) and first differences (bottom). However, there are several additional features worth noting. The upper graph shows several downward spikes, which represent strong deviations from the prior three months of language use. In several cases, this is followed by a rise back to relative consistency suggesting that the change in language was incorporated and adopted. In other words, some industry shift took place, which then became a permanent feature of the lexicon.

One notable exception are the years 1999 through 2001 during which the consistency of the language fluctuates several times. This is most apparent when looking at the (lower) graph in first differences around this time. Clearly the the volatility of this measure is pronounced. This coincides with the general turmoil surrounding the Dotcom bubble and the subsequent spillover into other industries. For instance, many biotech firms went out of business during this period for lack of financing while others sought the safety of alliances with large pharmaceutical companies (Wolfe 2001).

%\begin{landscape}
\begin{figure}
%\begin{figure}[p!]
\begin{center}
\includegraphics[scale=.4]{../figures/lcon2.png}
\caption[Monthly Language Consistency, Levels and Differences (1993-2004)]{Monthly Language Consistency in Levels and First Differences (1993-2004)\label{lcon2}}
\end{center}
\end{figure}
%\end{landscape}

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[scale=.65]{../figures/lcon3-edit.png}
\caption[Highlighted Deviations in Monthly Language Consistency]{Highlighted Deviations in Monthly Language Consistency\label{lcon3}}
\end{center}
\end{figure}
\end{landscape}

Figure \ref{lcon3} highlights the most significant drops in consistency over the 12 year period (1993-2004) after which there appears a period of relative consistency in the language. The three highlighted deviations occurred in March 1994, January 1997 and July 2001. Listed next to each deviation are the 25 word-stems, which contributed most significantly to the change in probability. The word-stems with positive change (increase in probability mass) are listed on top in green and those with negative changes (decreases in mass) are listed on the bottom in red. 

While it is difficult to draw strong conclusions, a few patterns are apparent. The deviation in 1994 was led by more  sharp declines than gains, with declines outpacing gains nearly 2.6 to 1. The declines suggest a waning attention to the early stages of biotech ventures with the words \emph{stock}, \emph{share}, \emph{chief}, \emph{executive}, \emph{announce} and \emph{agreement} all declining significantly. In contrast, the deviation in January of 1997 posts nearly even gains and declines with the words \emph{trial}, \emph{phase}, \emph{clinic}, and \emph{drug} heralding a possible shift towards the logic of commercialization as promising therapies made their way through the difficult FDA approval process. This shift is further supported by declines in the words \emph{study} and \emph{patent}. The decline of the word \emph{american} is also a signal of the globalization of the industry as previously mentioned.

The deviation in July of 2001 is again primarily driven by sharp declines in probability mass. While difficult to interpret from a reading of the word-stems, a review of the articles appearing around this time suggest a shift away from the science of biotech and towards the regulatory climate within which the industry was operating. This was shortly after George W. Bush started his first term as President of the United States and there was a concern amongst scientists that funding for stem cell research could be cut. This is supported by advances in the words \emph{stem} and \emph{grant}. There was also some discussion as to who would be approved to take over as head of the FDA. Although the word \emph{approve} didn't make the top 25 in absolute value (and thus is not shown in Figure \ref{lcon3}), it is in the top 20 words that made probability gains.


 
\section{Consensus and the Stock Market}

In Chapter \ref{law} I talked about the relationship between shared understanding (representational homology) and exchange.  I proposed that increases in shared understanding would drive exchange in markets characterized by a high degree of knowledge heterogeneity. I also proposed that new information (i.e. decreases in shared understanding) could drive exchange in markets that had reached a state of relative equilibrium. In this section I test these propositions by looking at how my measure of language consistency is related to the trading volume of common shares issued by publicly-traded biotech firms.

The set of firms used is taken from the dataset collected by Powell and his colleagues (see Powell et al. 2005; Powell, Koput and Smith-Doerr 1996), extended through 2004 and stock market data collected from the Wharton Research Data Service (CRSP/Compustat merged). Additional information on this dataset is presented in Chapter \ref{signal}; however a few points are worth mentioning here. The database focuses on dedicated human biotech firms, omitting companies involved in veterinary and agricultural biotech (which draw on different scientific capabilities and operate in a much different regulatory climate). The resulting sample covers 401 firms, of which 153 were in existence in 1993 and 245 in 2004. In this time-frame, some firms were created and entered the database, and others exited, due to failure, departure from the industry, or acquisition. 

To reduce volatility in my aggregate measure of trading volume and for econometric reasons clarified in Chapter \ref{signal}, I also dropped firms with less than three years of trading data from the sample. The final sample numbers between 111 and 192 firms over the years 1993 to 2004. Trading volume is operationalized as the log of the sum of the trading volume for all active firms in a given month. As such it captures general interest and activity in the biotech sector in proportion to firm size. This is in contrast to a measure like turnover, which captures valuation-independent trading activity. The two measures are related; however, in this context one can think of the volume measure like that associated with a sector mutual fund or portfolio, which are often size-weighted to begin with (Kim and Verrecchia 1991). Further analysis at the firm level can be found in Chapter \ref{signal}.

Figure \ref{vol1} shows standardized versions of trading volume and language consistency time series on the same axis. Despite significant volatility, both measures seem to adhere to the same long-term average. This is a striking feature of the data and suggests the two time-series may be cointegrated--that is, each may have a causal relation to the other as both conform to a longer-term trend. If trading volume deviates from the trend, it is ``pulled" back by language consistency just as consistency is ``pulled" back itself by higher trading volume. This adherence to the longer-term average and the interplay of the two variables can be more easily seen in Figure \ref{vol2}, which shows the 3-month moving average of the same variables. 


\begin{figure}
\begin{center}
\includegraphics[scale=.4]{../figures/vol1.png}
\caption[Standardized Trading Volume and Language Consistency]{Standardized Trading Volume and Language Consistency\label{vol1}}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=.4]{../figures/vol2.png}
\caption[Standardized Trading Volume and Language Consistency (3-month MA)]{Standardized Trading Volume and Language Consistency (3-month MA)\label{vol2}}
\end{center}
\end{figure}

When analyzing two time-series variables that appear nonstationary, it is common practice to look at their relationship in first differences, which are assumed to be covariance stationary (Phillips 1986). However, if the variables cointegrate (as I suspect given the plots), then even a simple regression using first differences can be misspecified yielding spurious results (Granger and Newbold 1974). In these cases it is important to test wether some linear combination of the series is stationary even though each series is not stationary by itself (Granger and Engle 1987). Following the work of Trusov, Bucklin and Pauwels (2007), Xiong and Bharadwaj (2013) and others I first use Aikake Information Criterion to identify the lag order of the combined series, which is three in this case. I then test for cointegration using the methods described in Johanson (1988, 1995) with three lags. The trace statistic of $0.3018$ is well below the $5\%$ critical value of $3.76$ used for a regression with 13 parameters, thus confirming cointegration.

In the presence of cointegration, the familiar vector autoregression concept (see e.g. Trusov, Buckling and Pauwels 2007) can be extended to a vector error-correction model, or VECM. The model is fit to the first differences of the nonstationary variables, but a lagged error-correction term is added to account for the long-term relationship. In the case of two variables, the added term is the lagged residual from the cointegrating regression of one of the series on the other in levels. Thus, it expresses the prior disequilibrium from the long-run relationship, in which that residual would be zero.



\begin{enumerate}
  \item Show patterns with trading volume
  \item Show statistical analysis of relation to trading volume
  \item However, confounded with macro economic changes
  \item However, confounded with relationship to systematic risk
  \item Show short term reactions and interdependencies.
  \item However, there may be significant heterogeneity across firms
  \item This will be handled in next section
 \end{enumerate}


