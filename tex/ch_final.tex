\chapter{Final Thoughts \label{final}}


\begin{small}
\begin{quote}
``...perhaps we should follow Simon's lead and look more closely at the process of decision making. Instead of beginning our analysis by assuming a set of options we should enquire what options come to be included in the choice set--for, as Alchian recognized, the optimum cannot be chosen unless it has been thought of" \citep[p. 31]{loasby1999}. 
\end{quote}
\end{small}

\section{Summary of Findings}

As Loasby in the above quote recognizes, decisions in the marketplace (and the ensuing exchange) are linked to the finite knowledge and understanding of market actors. This occurs because individuals have limited cognitive resources and are heterogeneous in the use of these resources \citep{simon1957}. In Chapter \ref{law}, I introduced a concept called market entropy as a way to describe the degree of this knowledge heterogeneity in a market. In low entropy, alpha markets, we can reliably distinguish between the representations of various actors selected at random. In high entropy beta markets, most actors share similar representations.

I proposed that in alpha markets, exchange would follow an increase in representational homogeny. This proposition was supported in Chapter \ref{lang} by showing that the trading volume of biotech firms increased over the long term with increases in the consistency of language. However, the time-series were cointegrated, such that language consistency responded to increased trading volume as well. The codependency of these two constructs has intuitive appeal.  Just as greater shared understanding can promote exchange, the exchange process itself can result in more shared understanding.

I also found that short-term decreases in language consistency (i.e. new information) was predictive of transient increases in trading volume. This supported the second proposition of Chapter \ref{law} in which I suggested that reductions in entropy would prompt exchange when markets had reached a state of relative equilibrium. Whereas the effect of consistency was long-term and trending, the effect of inconsistency was short-term and stationary.

In Chapter \ref{signal} I asked how the knowledge heterogeneity concept developed in Chapter \ref{law} and operationalized in Chapter \ref{lang} would affect the performance of market actors. By integrating knowledge-based theories of the firm \citep[e.g.][]{powell1996} and the work of \citet{podolny2005} on structural market signals, I offered a description of performance based on both the substantive and symbolic advantages of network position. The uncertainty caused by high levels of knowledge heterogeneity positively moderated the symbolic advantage of network position but negatively moderated the substantive advantage.

\section{Broad Implications}
The proposed shared representations model of exchange, an extension of Alderson's law of exchange, has multiple implications for the practice of marketing. In that regard, one implication appears foremost--an understanding of market entropy should precede the formulation of marketing strategy. This is because the characteristics of alpha and beta markets (and the extent to which a market is trending one direction or the other) can inform the type of strategy that should be pursued. In brief, when market entropy is high (undifferentiated), strategy should orient towards innovation and novelty. In contrast, when market entropy is low (highly differentiated), strategy should orient towards diffusion and similarity.  

The practice of marketing is further affected by the focus on shared representations (representational homogeny) as a foundational concept of interest. It moves the marketing discipline away from a product centric focus, and (to some degree) away from a customer focus. In their place is a focus on generic market actors and generic market offerings. To illustrate this type of thinking I can ask for instance: ``how does the marketplace represent an automobile, cell phone, or college?" The focus, at least initially, is not on the brand--as in Mercedes, iPhone or Harvard--but rather on the way in which market actors represent the fundamental concepts of transportation, communication and education. Do their representations overlap? Where are they different? What opportunities exist in the answers to these questions?     

\textbf{Innovation.}
When should a firm attempt incremental vs. radical or disruptive innovation? From an abstract and propositional knowledge perspective, this question was previously answered. Proposition 1 in Chapter \ref{law} suggests that increasing representational homogeny (similarity) will drive exchange when market entropy is low. Thus, in highly differentiated (alpha) markets, exchange will be driven by incremental innovation. On the other hand, Proposition 2 recommends the introduction of new representations (i.e. novelty) or what can be thought of as radical design when market entropy is high. Thus, in undifferentiated (beta) markets, exchange will be driven by more aggressive innovation. 

New product and service innovations often exist at the confluence of two previously unrelated industries \citep{arthur2009}. Consider for instance, the recent emergence of the so-called ``sharing economy." Services like Uber\footnote{http://www.uber.com}, RelayRides\footnote{http://www.relayrides.com} and AirBNB\footnote{http://www.airbnb.com}  have succeeded largely by merging developments in information technology with traditional offerings in transportation and hospitality. Yet, the ``disruption" of these beta markets necessarily required a new representation of the market for cabs, rental cars and hotels. The resultant increase in exchange stems from an initial decrease in market entropy followed by the gradual acceptance by market actors of the process through which these services operate. Newer services like tool-sharing service Streetbank\footnote{http://www.streetbank.com} have subsequently leveraged the market representation of a ``sharing economy" to expand their business as entropy increases and the market again equilibrates to beta. 

\textbf{Marketing Communication.}
In practice, material innovation considered ``radical," can be quite difficult to achieve. This is especially evident in markets defined by commodity resources like gasoline or aspirin.  When confronting these undifferentiated (beta) markets, communication strategies that emphasize the uniqueness of market offerings can help drive exchange. Since the material market offerings are undifferentiated, marketers should foster uniqueness by focusing on intangibles such as brand image. Intangibles are often more difficult for competitors to copy \citep{teece1986} and can thus better sustain exchange in a beta market. For instance consider aspirin, baby oil, blue denim jeans, gasoline, or many kitchen appliances. The dissimilarity of offerings in each of these markets is the result of considerable investments in brand image. Consequently the brands of Bayer, Johnson \& Johnson, Levi, Shell, and Kitchen Aid constitute a source of differentiation in an otherwise beta market.  

In contrast, marketers dealing with highly differentiated (alpha) markets, should use communication strategies that emphasize the similarity of market offerings. Granted, some differentiation can remain (e.g. a lower price); however, a competitive market offering must be substitutable by definition. Recall from the discussion in Chapter \ref{law}, that recognition of element differences are (paradoxically) based initially on some notion of equivalence. If a competitor offers a lower price, the rest of their market offering must be similar enough to the offering with a higher price, that the difference can be meaningfully evaluated. In an alpha market, this baseline for comparison across market offerings cannot be taken for granted.

\textbf{Supply Chains and Channels.}
Marketing management develops strategy and tactics for the flow of goods and services between market actors. Traditionally, this ``flow" has been represented as a series of dyads that result in a chain or channel. However, increasingly scholars are applying concepts like value constellations \citep{normann1993}, actor-to-actor networks \citep{vargo2011}, and ecosystems \citep{mars2012} to solve the problems of supply and distribution. This broadened perspective fits well with the representational homogeny view of markets. Shared representations of the marketplace constitute the glue, which holds these constellations, networks, and ecosystems together. As \citet{vargo2011} suggest, shared institutional logics provide the norms, rules and customs that facilitate trust and thus allow actors to efficiently cooperate in the exchange of service. However the level of efficiency realized depends on the level of representational homogeny present in the exchange context. Trust can be defined as the certainty one actor has about the behavior of another.\footnote{Granted, some would argue that trust can only have positive valence, which negates this definition. However, I use `trust' in a broader sense, in that one can \emph{trust} some market actor to act a certain way regardless of the quality of that action. Trust is therefore synonymous with predictability.} As argued previously, this certainty is dependent on a shared understanding of the rules that govern exchange behavior. 

\section{Future Research}
Virtually all of the preceding implications for marketing practice require new research and measurement techniques/tools. Foremost, we need to be able to answer the following, ``how does one go about measuring the concept of representational homogeny and hence market entropy?" I presented one approach in Chapter \ref{lang}; however I offer some additional suggestions and also recommend further work that needs to be done to more fully answer this important question. 

\textbf{The Entropy Concept and Measures.}
A preliminary but vital question is if entropy is an appropriate measure to capture representational homogeny. I believe it is a valid measure because it captures variation independent of scale and it is grounded in theory. In this regard I am not relying on the concept of entropy from thermodynamics, which posits that in a closed system energy will be used up as entropy inevitably increases. Notably, some economists \citep[see e.g.][]{georgescu1971, rifkin1980} and macro-marketers \citep{kilbourne1997} are focused on this concept of entropy. My focus is instead on the notion of entropy as developed in information and communications theory. This is appropriate because it allows for new information in the form of surprise or novelty and thus the reduction of entropy (the creation of order) as a counterpoint to the absence of information (max entropy). Since market representations are not static, this conceptualization also allows us to capture the non-equilibrium nature of markets in practice--the notion that markets generally move between order and disorder in response to information shocks \citep{hunt1999, dickson1992}. 

The entropy concept, as I use it, is not new to marketing and has already provided valuable insights. For instance \citet{ramaswamy1993} use an entropy measure to determine if market segments are reliably distinct. They find that when entropy is at its maximum it is not possible to make reliable distinctions between segments. Additional research using an entropy measure in market segmentation studies has produced similar results \citep{desarbo1995, kamakura1995, hofstede1999}. Further research suggests entropy as the primary diagnostic statistic to ensure separation of parameters defining segments \citep{kamakura1995}. My model complements that of \citet{ramaswamy1993} and subsequent researchers by noting that in high entropy, undifferentiated (beta) markets, the representations of market actors are highly homogeneous. I depart from prior research in my use of the entropy concept to motivate theory on exchange.

\textbf{Fine vs. Coarse Grain Measurement.}
It is unrealistic and unnecessary to examine every minor nuance of a representation. Thus, I suggest identifying the key elements (or features) of a shared representation. This process of ``coarse graining,"--while often involving substantial intuition on the part of the researcher--is one about which social scientists are keenly familiar \citep{klingenstein2014, stewart1981}. A key element is one that if it were not included in the final formulation, the representation would have substantially less meaning and, thus rendered unrecognizable by market actors. Consequently, a priority in developing measurement models of shared representations is to identify how to meaningfully isolate the key elements. In Chapter \ref{lang}, I used shifts in the frequency of descriptive words; however, future work could look at synonym sets or other theoretically-driven data reduction strategies \citep{klingenstein2014}.

At the most fine-grained level, elements that comprise a representation are very numerous, and yet, may not be noticeable by humans. Consider for instance, the representation of red wine. With the exception of professional wine tasters, most individuals are unable to tell the difference between an \$8 bottle and a \$25 bottle. In product testing laboratories one can use electron microscopy to view defects--for example, in leather used to cover a car seat. While these defects may not be visible to most humans, it is possible that such microscopic elements collectively create an overall impression of quality--especially when combined with other design elements. For instance in the leather car seat example, if one combines a representation of the car flooring, side panels and dash, then a more ``coarse-grained" representation of luxury may be formed. Understanding the separation between fine and coarse-grained representations is crucial for undertaking research in this area.

\section{Practical Implications}
Taken together my findings have certain practical implications for managers. Foremost, shared understanding appears to increase exchange over the long term. Moreover, the construct may be reliably accessible via analysis of written language. Tracking this construct using the operationalization of Chapter \ref{lang} could be a useful forecasting metric. For instance, managers interested in the volume of activity within their industry may consider use of language consistency as a leading indicator.\footnote{Some findings in related work still under development suggests this measure is also predictive of systematic risk.} A natural extension of this finding is to look at less formal text like that generated on product review websites or Twitter to see if user-generated content has the same predictive capacity. This would also allow for measures to be delivered to managers as the text naturally occurs in real time.

Not only does shared understanding promote exchange, it differentially \emph{directs} exchange. When language uncertainty was low (i.e. high degree of shared understanding), exchange was directed at less central market actors. When uncertainty was high, exchange was concentrated towards an industry's central actors. And yet, the fruits of this exchange were more difficult for these central actors to use productively. 

These findings can provide strategic guidance for both entrepreneurs and established firms. Under high language uncertainty, entrepreneurs may have a more difficult time securing the resources necessary to survive. However, they can compete on more equal footing with central actors in the use of resources they \emph{do} have. This is because the central actors are unable to fully capitalize on the information flowing across network ties. In such a case, entrepreneurial firms that have sufficient resources (e.g. financing), may outperform their peers. Those that do not, should focus their efforts on securing enough resources to survive until a point at which their underlying quality is more easily recognized.

In contrast, \emph{new} resources are distributed more broadly under low language uncertainty (i.e. high consistency). In this case, central firms should continue to use their networks to gain an information advantage. Entrepreneurial firms should focus their efforts on forming alliances of their own that can provide access to similar information.

While there is still much to be done in the future, the goal of the current work has been to provide a unified study of the knowledge-exchange relationship. I approached the the problem using theoretical arguments in Chapter \ref{law}, descriptive and aggregate, causal inference in Chapter \ref{lang} and specific hypothesis testing at the firm-level in Chapter \ref{signal}. Each of these chapters built upon each other in what I hope stands as a significant contribution to our understanding of this complex issue.

